---
title: 02/08 Summary
lang: ko-KR

meta:
  - name: description
    content: 부스트캠프 AI Tech 02/08 Summary
  - property: og:title
    content: 02/08 Summary
  - property: og:description
    content: 부스트캠프 AI Tech 02/08 Summary
  - property: og:image
    content: 'https://boostcamp.connect.or.kr/images/pavicon_180_v2.ico'
  - property: og:url
    content: https://leedooho.com/AI/Boostcamp-AI-Tech/0208.html
---

### 2022/02/08, 새로 알게된 점

<p class="tags">#Convolution #CNN #1x1_Convolution #Semantic_Segmentation #Object_Detectoin #RNN #LSTM #GRU</p>

#### Convolution

$$\underset{\text{Continuous convolution}}{(f\ast g)(t)\;=\;\int f(\tau)g(t\;-\;\tau)d\tau\;=\;\int f(t\;-\;\tau)g(t)d\tau}$$

continuous convolution 은 2개의 함수를 **잘 섞어주는** operator 로 정의한다. discrete 버전은 아래의 식으로 표현된다.

$$\underset{\text{Discrete convolution}}{(f\ast g)(t)\;=\;\sum_{i=-\infty}^{\infty}f(i)g(t\;-\;i)\;=\;\sum_{i=-\infty}^{\infty}f(t\;-\;i)g(i)}$$

우리가 알고 있는 2D image convolution 은 다음과 같이 표현한다. $I$ 가 **전체 이미지 공간**이 되고, $K$ 가 우리가 적용하고자 하는 **filter** 이다.

$$\underset{\text{2D image convolution}}{(I\ast K)(i,\;j)\;=\;\sum_{m}\sum_{n}I(m,n)K(i-m,j-n)\;=\;\sum_{m}\sum_{n}I(i-m,i-n)K(m,n)}$$

2D convolution 은 fiter 를 이미지에 찍는다고 생각할 수 있는데, 같은 이미지에 **filter 모양에 따라서 Blur, Emboss, Outline 등의 효과**가 나타날 수 있다.

&nbsp; 이제 channel 도 고려해보자. Grayscale Image 가 아닌 RGB Image 를 다루는 이상 channel 은 3이 된다. 2D covolution 을 다룰 때 filter 는 2차원이라고 했는데, 
어떻게 되는 것일까? 답은 해당 **filter 가 모든 channel 마다 적용이 되는 것**이다. 하지만 일반적으로 딥러닝에서 convolution 을 사용한다면, 
**이미지의 크기를 줄이며** <span style="color: #2454ff;">**정보를 축약**</span>하는 방향으로 간다. 
그리고 <span style="color: #2454ff;">**convolution 을 거친 Image 의 channel 은 filter 의 개수**</span>와 같다.

&nbsp; 다시 Neural Networks 에서 convolution 이 가지는 의미를 살펴보자면 다음과 같다.<br>
<span style="color: #2454ff;">**Convolution Layer**</span> 와 <span style="color: #2454ff;">**Pooling Layer**</span>, 
그리고 <span style="color: #2454ff;">**Fully connected Layer**</span> 를 합쳐 <span style="color: #2454ff;">**CNN**</span> 이라고 부른다. 
Convolution 과 Pooling Layer 를 통해 <span style="color: #2454ff;">**feature extraction**</span> 을 진행하고, 
Fully connected Layer 를 통해 달성하고자 하는 목표를 정한다. (classification, ...)

&nbsp; 이외에도, 얼만큼의 Pixel/정보를 건너뛰는가에 대한 <span style="color: #2454ff;">**Stride**</span>, 
boundary 정보를 살리기 위해 여백을 얼마나 주는지에 관한 <span style="color: #2454ff;">**Padding**</span>도 있다. 
이러한 정보를 토대로 학습을 통해 filter 속 최적의 weights 을 찾는 것이다. 파라미터 수를 계산하는 방식은 
<span style="color: #2454ff;">**Filter Size x Input Channel(kernel) x Output Channel(kernel)**</span> 이다. 
channel 이 128인 image 에 3 x 3 filter 64 개를 적용한다면, 학습해야 하는 파라미터는 3 x 3 x 64 x 128 = **73,728** 개이다...

&nbsp; 최근의 추세는 파라미터의 개수를 줄이기 위해 FC (Fully Connected Layer) 를 줄인다. 파라미터가 많을 수록 학습시키기 어렵고, Generalization 이 잘 안된다. 
목표는 <span style="color: #2454ff;">**Layer 를 깊게 쌓음과 동시에 파라미터 수를 줄이는 것**</span>이다. 
그래서 앞단에 CNN 을 깊게 쌓고, 마지막 FC 를 최소화하려고 한다. 이처럼 항상 모델을 설계할 때 파라미터 수를 염두하자.

<br>

#### 1x1 Convolution

&nbsp; 주위의 pixel 을 고려하지 않고 <span style="color: #2454ff;">**채널을 줄이기 위해**</span> 주로 사용된다. 
채널을 줄임으로써 <span style="color: #2454ff;">**파라미터의 수를 줄일 수 있다**</span>. 예시를 들어 살펴보자.

&nbsp; 큰 사이즈의 128 Channel 에서 작은 사이즈의 128 Channel 로 만들어야 한다고 생각해보자. 3x3 filter 를 사용할 경우, 
3 x 3 x 128 x 128 = **147,456** 개의 파라미터가 필요하다. 
다른 방법으로, 큰 사이즈의 128 Channel 에서 사이즈를 유지한 채, 
<span style="color: #2454ff;">**1x1 Convolution 을 사용하여 32 Channel 로 줄이고**</span>, 
다시 3x3 Convolution 으로 작은 사이즈의 128 Channel 로 바꾸는 방식으로 해보자. 
(1 x 1 x 128 x 32) + (3 x 3 x 32 x 128) = **40,960** 개의 파라미터로 약 <span style="color: #2454ff;">**72% 줄였다**</span>.

&nbsp; 이런 방식을 **Bottleneck Architecture** 이라 부른다. 1x1 Convolution 을 적용하면서 점차 깊어지고 파라미터 개수는 줄어드는 CNN 모델의 흐름을 살펴보자. 

<br>

1. **AlexNet** (60M)

    - 11x11x3 filters 로 시작하는 **5개의 convolutional layers** 와 **3개의 dense layers** 로 구성되어 있다.
    - 이미지 분류에서 우승할 수 있었던 점으로 첫번째, <span style="color: #2454ff;">**ReLU**</span> Activation function 을 사용했다는 점이다. 
    - 두 번째로는, **2개의 GPU**를 사용하였다.
    - 세 번재로는 **Local response normalization** 과 **Overlapping pooling** 을 사용하였다.
    - 이 외에는 **Data augmentation** 과 **Dropout** 을 사용하였다.
    - ReLU 를 사용함으로써 <span style="color: #2454ff;">**GD를 사용한 최적화**</span>가 쉬워졌고, 
    <span style="color: #2454ff;">**Generalization**</span> 이 좋아졌다. 
    또한 <span style="color: #2454ff;">**Vanishing Gradient 문제를 어느정도 극복**</span>하였다.
        - weights 의 값이 0보다 커지면, sigmoid 나 tanh 를 activation function 으로 사용할 경우 slope 이 0에 가까워져 Vanishing Gradient 문제가 발생하였다.
    - Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton, "ImageNet Classification with Deep Convolutional Neural Networks", NIPS, 2012

<br>

2. **VGGNet** (110M)

    - stride = 1 인 <span style="color: #2454ff;">**3x3 filter 만을 사용**</span>하였다.
        - 3x3 filter 를 두 번 적용하면 5x5 filter 를 한 번 적용한 것과 같다! **Receptive field** (최종) 차원으로는 동일하지만, 
        <span style="color: #2454ff;">**더 적은 파라미터로 동일한 효과**</span>를 낼 수 있다!
        - 그래서 이후에 나오는 대부분의 모델들은 7x7 filter 를 벗어나는 크기를 사용하지 않는다.
    - FC 를 위해 **1x1 convolution** 을 사용하였지만, 파라미터를 줄이기 위해 사용한 것은 아니다.
    - **Dropout(0.5)** 를 사용하였다.
    - Layer 개수에 따라 VGG16, VGG19 라고 부른다.
    - Karen Simonyan, Andrew Zisserman, "Very Deep Convolutional Networks for Large-Scale Image Recognition", ICLR, 2015

<br>

3. **GoogleNet** (4M)

    - 총 **22개의 Layers** 로 이루어져 있으며, 내부적으로 **비슷한 네트워크가 반복**적으로 사용된다.
        - 네트워크 안에 네트워크가 있다고 해서 <span style="color: #2454ff;">**NiN 구조**</span>라고 한다.
    - 2014년에 ILSVRC 대회를 우승하였고, inception block 을 NiN 구조로 사용하였다.
        - Inception block 은 Input 이 여러 갈래로 나뉘었다가 다시 합쳐지는 구조이다. 중요한 점은 내부에 3x3, 5x5 convolution 을 적용하기 전에, 
        <span style="color: #2454ff;">**1x1 convolution 을 사용**</span>한다! 이를 통해 파라미터 개수를 줄일 수 있다.
    - Christian et al. "Going Deeper with Convolutions", CVPR, 2015

<br>

4. **ResNet**

    - Layer 를 많이 쌓을수록 학습시키기 어렵다. 많은 파라미터로 인해 Overfitting 이 발생하여, 학습이 되질 않는다. 이것을 해결하기 위해 
    <span style="color: #2454ff;">**Skip Connection ( $f(x)\;\rightarrow\;x+f(x)$ )**</span> ( **+ Identity Map** ) 을 추가하였다.
        - Skip Connection 의 의미는, 전 Layer 와의 <span style="color: #2454ff;">**차이**</span>, 
        x 만큼에 대해서 <span style="color: #2454ff;">**학습**</span>하겠다는 것이다.
    - Simple Shortcut (Skip Connection) 전후로 차원이 맞지 않는 경우에는, <span style="color: #2454ff;">**1x1 Convolution**</span> 을 사용(Projected Shorcut)하여 
    차원을 맞췄다.
    - <span style="color: #2454ff;">**Bottleneck Architecture**</span> 를 사용하였다.
    - ResNet 에서는 **3x3 Convolution > Batch Norm > ReLU** 순으로 적용하였다. (하지만 Batch Norm 적용 순서에 대해서는 논란이 많다.)
    - Kaiming He, Xiangyu Zhang, Shaoquing Ren, Jian Sun, "Deep Residual Learning for Image Recognition", CVPR, 2015

<br>

- 기타 **DenseNet**

    - ResNet 은 Skip Connection 에서 더하는 방식을 사용했다면, DenseNet 에서는 합치는 ( <span style="color: #2454ff;">**concatenation**</span> ) 방식을 사용하였다.
    - 하지만 합치기만 하면 Layer 를 쌓을 수록 기하급수적으로 Channel 이 증가하고, 파라미터는 더더욱 많아진다. 그래서 중간에 계속해서 줄여줘야 한다.
    - <span style="color: #2454ff;">**Dense Block**</span> 으로 합치며 Channel 을 늘리고, 
    **Batch Norm > 1x1 Convolution > 2x2 AvgPooling** 방식인 <span style="color: #2454ff;">**Transition Block**</span> 을 통해 차원을 줄인다.

<br>

#### Semantic Segmentation

&nbsp; Sematic Segmentation 이란 이미지 속 **Pixel 마다 분류**하는 것이다. 어려운 문제이다.<br>
지금까지는 Image 가 들어오면 Convolution 및 Pooling 을 거치고 마지막에는 flat 하게 펼쳐 FC Layer 를 거친다. 
여기서 우리는 FC 인 Dense Layer 를 없애기 위해 <span style="color: #2454ff;">**Fully Convolution**</span> 를 사용한다. 
Pixel 마다 분류를 위해 마지막을 Convolution 으로 바꾼것 뿐이지, <span style="color: #2454ff;">**파라미터와 output 차원은 동일**</span>하다.

&nbsp; 이러한 Fully Convolutional 작업을 하는 이유는, 첫 번째로 <span style="color: #2454ff;">**Input 차원에 독립적**</span>이라는 점이다 
(filter 가 Image **크기에 상관없이 동일하게 찍어내기 때문**). 그리고 두 번째로, 분류만 했던 작업에서 <span style="color: #2454ff;">**Heatmap**</span> 을 그릴 수 있다.

&nbsp; 이렇게 **FCN** (Fully Convolutional Network) 가 Input 차원에 독립적이라고 하지만, **Output 차원은 줄어들게 된다** (by subsampling). 
**Resolution 이 떨어진 Output** (coarse output) 을 다시 **dense** 하게 만들어줘야 한다 (upsampling).

&nbsp; upsampling 방식으로 <span style="color: #2454ff;">**Deconvolution**</span> (convolution transpose) 이 나오게 되었다. 
Convolution 을 역으로 계산하는 것으로, **공간의 차원을 늘려주는 역할**을 하게 된다. 그런데 생각해보면, Convolution 과정은 정보를 축약하는 것으로 
정보의 손실은 반드시 존재한다. 그래서 <span style="color: #2454ff;">**정확히 복원할 수는 없다**</span>. 
Deconvolution 은 입력 차원에 padding 을 줘서 원하는 output 차원으로 만드는 정도의 역할을 한다.

※ Input Image &nbsp; &#10140; &nbsp; Groud-truth > FCN > DeconvNet > EDeconvNet > EDeconvNet + CRF

<br>

#### Object Detection

&nbsp; 이제 볼 <span style="color: #2454ff;">**Detection**</span> 문제도 유사하다. 
이 문제를 해결할 수 있는 가장 간단한 모델은 <span style="color: #2454ff;">**R-CNN**</span> 일 것이다.

1. **R-CNN**

    - Image 안에서 약 2,000 개의 <span style="color: #2454ff;">**region**</span> 을 무수히 뽑는다 (Selective Search Algorithm). 
    - 그리고 CNN 에 입력하기 위해 다양한 크기의 region 을 동일하게 맞춘다. 그런 다음 <span style="color: #2454ff;">**SVM**</span> 을 이용하여 
    <span style="color: #2454ff;">**분류**</span>를 진행한다.
    - 추가적으로 **Bounding Box Regression** 이라고 Bounding Box 를 어떻게 옮겨야 좋을지 같이 찾는 문제가 들어간다. 
    - 위의 과정을 거치면 대략적으로 물체가 어디에 있는지 bounding box 로 표시할 수 있다.

<br>

2. **SPPNet** (Spatial Pyramid Pooling)

    - R-CNN 의 문제라고 한다면, 너무 많은 patch (region) 을 뽑는다는 점이고, 이 모든 region 을 CNN 에 통과시켜야 한다. 
    이러면 CPU 에서 **한 이미지**를 처리하는데 약 **1분**이 걸린다...
    - R-CNN 을 개선하여, CNN 을 한 번 통과하여 **Convolutional Feature Map** 을 만든 다음에, region 별 Bounding Box 위치에 해당하는 **tensor** 만 가져온다. 
    그래서 훨씬 빠르다.

<br>

3. **Fast R-CNN**

    - SPPNet 도 여전히 느리다고 생각했다. CNN 은 한 번 통과하지만, 여전히 많은 Bounding Box 에 대해서 분류작업을 진행해야 하기 때문이다.
    - 방법을 바꿔서, Input Image 에 대해서 2,000 개 가량의 Bounding Box 를 뽑는다. 그리고 CNN 을 한 번 통과한다. 그런 다음, 
    각각의 region 에 대해서 **fixed length feature** (RoI)를 뽑는다. 
    마지막으로 Neural Network 를 통해서 Bounding Box Regression 을 적용한다.
    - SPPNet 과 유사하지만 마지막 Neural Network 를 통해 Bounding Box Regression 과 Classification 을 했다는 차이가 있다.

<br>

4. **Faster R-CNN**

    - R-CNN 계열의 마지막 모델이다. <span style="color: #2454ff;">**Bounding Box 를 그려내는 것 조차도 학습**</span>을 시킨다. 
    ( **Region Proposal Network** )
    - RPN (Region Proposal Network) 의 역할은 특정 영역인 Patch 가 Bounding Box 안에 있을지 없을지 판단하는 것이다. 이 물체가 무엇인지는 뒷단의 
    네트워크가 찾는다.
        - 미리 정해진 Bounding Box 템플릿인 <span style="color: #2454ff;">**Anchor Boxes**</span> 을 활용하여 찾는다.
        - Fully Convolution 을 사용한다. &#10140; 모든 영역에 대해 계산 &#10140; 해당 영역에 물체가 있는지 없는지를 학습
            - 128, 256, 512 라는 각각의 다른 region size 와 1:1, 1:2, 2:1 이라는 다른 비율의 조합으로, 총 <span style="color: #2454ff;">**9개의 다른 모양**</span>을 가진다.
            - 그리고 각각의 region 마다 사이즈를 키울지 줄일지 (x, y) 판단하는 <span style="color: #2454ff;">**파라미터 4개**</span>가 필요하다.
            - 마지막으로 해당 bounding box 가 쓸모 있는지 없는지 판단하는 <span style="color: #2454ff;">**2개의 파라미터**</span>가 필요하다.
    - 성능이 꽤나 잘 나온다.

<br>

5. **YOLO** (You Only Look Once)

    - v5 버전까지 나왔지만 v1 만 살펴보자. YOLO 는 <span style="color: #2454ff;">**매우 빠른**</span> Object Detection Algorithm 으로, 
    기본 45fps, 작은 버전은 155fps 추론 속도를 보인다.
    - 이전에 RPN 과 뒷단의 네트워크를 쓰는 방식이 아닌, Image 에 바로 모든 것을 적용한다. 여러 bounding box 와 분류 문제를 한 번에 진행한다.
    - Input Image 를 SxS Grid 로 나눈다. 찾는 물체의 중앙이 Grid 안에 들어가면, 해당 물체가 Bounding Box 안에 들어가는지와 어떤 물체인지 같이 찾는다.
        - 각각의 cell 은 B (default=5) 개의 bounding box 를 찾는다. 그리고 해당 Box 들이 쓸모 있는지 없는지 같이 찾는다.
        - 이와 <span style="color: #2454ff;">**동시**</span>에 해당 cell 이 어떤 클래스인지 예측한다.
        - Tensor &nbsp; &#10140; &nbsp; <span style="color: #2454ff;">**S x S x (B*5 + C)**</span> size &nbsp; (C = Number of classes)

<br>

#### RNN

&nbsp; 이제 CNN 을 넘어서 RNN 인데, 그전에 **Sequential Model** 이 무엇인지 알아보자.

&nbsp; Sequential Data 는 일상 생활에서 접하는 대부분의 데이터라고 볼 수 있다. Audio, Video, Motion 등 순서 정보가 담긴 데이터인데, 
Sequential Data 를 처리하는 것은 까다롭다. 받아들여야 하는 Input 차원을 특정하기 힘들기 때문이다. 

- Naive sequence model

$$p(x_t\;|\;\overset{\text{The number of inputs varies}}{x_{t-1},\;x_{t-2},\;...})$$

그래서 몇 개의 과거 데이터만 본다고 특정하면 계산하기 쉬워진다.

- Autoregressive model

$$p(x_t\;|\;x_{t-1},\;...,\;\overset{\text{Fix the past timespan}}{x_{t-\tau}})$$

더 나아가, 현재는 (바로 전) 과거에만 연관된다고 가정하여 아래의 모델이 나왔다. 
Joint Distribution 을 잘 표현하지만, 앞서 언급한 가정으로 인해 많은 정보를 잃어버린다. 

- Markov model (first-order autoregressive model)

$$p(x_1,...,x_T)\;=\;p(x_T|x_{T-1})p(x_{T-1}|x_{T-2})\dots p(x_2|x_1)p(x_1)$$
$$=\;\underset{\text{Easy to express the joint distribution}}{\prod_{t=1}^{T}p(x_t|x_{t-1})}$$

그래서 이러한 방식을 이용하지 않고, 다른 방식을 사용한다. 이전 모델들은 과거의 많은 정보를 제대로 활용하지 못한다는 문제가 있는데, 
소개할 아래의 모델은 중간에 <span style="color: #2454ff;">**Hidden state**</span> 을 두어, 과거의 정보를 요약하고 이를 사용한다. 
그리고 다음 Step 은 직전의 Hidden State 에만 연관된다.

- Latent autoregressive model

$$\hat{x}\;=\;\overset{\text{summary of the past}}{p(x_t|{\color{red}{h_t}})}$$

$$h_t\;=\;g({\color{red}{h_{t-1}}},\;x_{t-1})$$

<br>

&nbsp; 앞서 언급한 모델들의 내용을 잘 요약한 것이 RNN (Recurrent Neural Network) 이다. 
기존 MLP 와 동일하지만 차이라면 **자기 자신으로 돌아오는 구조**가 있다는 것이다. 
Sequential Data 가 순차적으로 Input 에 들어가며, Weight 와 곱해서 나온 값과, 이전 **cell state** 의 값을 합쳐서 (concatenate) tanh 를 통과시킨다. 
이러한 과정을 반복하는 것이 RNN 이다. 하지만 RNN 에도 큰 단점이 있다.

&nbsp; 그 문제는 <span style="color: #2454ff;">**Short-term dependencies**</span> 라고, 과거의 정보를 취합하여 
미래에 사용되어야 하지만, 조금 떨어진 과거의 정보가 미래에 살아남기 힘들다는 것이다. 나중에 이것을 해결한 LSTM (Long-Short Term Model) 이 나오게 된다.

$$h_1\;=\;\phi(W^Th_0\;+\;U^Tx_1)$$

$$h_2\;=\;\phi(W^T\phi(W^Th_0\;+\;U^Tx_1)\;+\;U^Tx_2)$$

$$h_3\;=\;\phi(W^T\phi(W^T\phi(W^Th_0\;+\;U^Tx_1)\;+\;U^Tx_2)\;+\;U^Tx_3)$$

$$h_4\;=\;\underset{\text{Vanishing / exploding gradient}}{\phi(W^T\phi(W^T\phi(W^T\phi(W^Th_0}\;+\;U^Tx_1)\;+\;U^Tx_2)\;+\;U^Tx_3)\;+\;U^Tx_4)$$

데이터의 길이가 길 수록 gradient 가 0에 수렴 (Sigmoid) 하거나 폭발적으로 커지게 (RNN 을 쓸 때, ReLU 를 잘 사용하지 않는 이유) 된다.

<br>

#### LSTM

&nbsp; 이제 핵심 모델인 LSTM 이다. 각 어떤 요소가 있는지 알아보자.

- $x_t$ &nbsp; &#10140; &nbsp; Input 이다. (language model 이라면 단어가 될 것이다.)

- $h_t$ &nbsp; &#10140; &nbsp; Output 으로, Hidden State 라고 불린다.

- <span style="color: #2454ff;">**Previous Cell State**</span> &nbsp; &#10140; &nbsp; 지금 그림은 없지만, LSTM 도식 그림에서 상단 라인 부분이다. 내부에서만 흘러가고, 
0부터 지금(t)까지 (t+1 개)의 내용을 취합한 정보이다.

- Previous Hidden State &nbsp; &#10140; &nbsp; 하단 라인 부분이다. 다음번인 t+1 번째의 previous hidden state 로 들어간다.

- **Gate**

    - **Forget gate** &nbsp; &#10140; &nbsp; 버릴 정보를 결정한다. $f_t\;=\;\sigma(W_f\cdot[h_{t-1},x_t]\;+\;b_f)$
    - **Input gate** &nbsp; &#10140; &nbsp; cell state 에 저장할 정보를 결정한다.
        $$i_t\;=\;\sigma(W_i\cdot[h_{t-1},x_t]\;+\;b_i)$$
        
        $$\tilde{C}_t\;=\;\mathcal{tanh}(W_C\cdot[h_{t-1},x_t]\;+\;b_C)$$
    - **Output gate** &nbsp; &#10140; &nbsp; **updated cell state** 를 이용하여 output 으로 만든다.
        $$o_t\;=\;\sigma(W_o[h_{t-1},x_t]\;+\;b_o)$$

        $$h_t\;=\;o_t\ast \mathcal{tanh}(C_t)$$

<br>

#### GRU

&nbsp; 추가적으로 GRU (Gated Recurrent Unit) 도 알아보자.<br>
LSTM 의 Gate 가 3개였다면, GRU 의 Gate 는 **2개**이다 (reset gate, update gate). **cell state 가 없으며**,
<span style="color: #2454ff;">**hidden state 만 존재**</span>한다.

&nbsp; LSTM 에서 성능이 좋지 않았지만 GRU 를 통해 성능이 증가한 것도 있으니 알아두자. (결국에는 **Transformer** 를 주로 사용하게 될 것이다...)

<br>

<hr>

### 피어 세션

앞으로 모델 제작할 때 파라미터를 간략하게나 계산하면서, 또 print 모델 파라미터 찍어가며 하자!!

<br>

<br>

<br>

<style scoped>
.tags { color: #2454ff; }
a { color: #2454ff; }
</style>