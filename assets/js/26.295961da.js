(window.webpackJsonp=window.webpackJsonp||[]).push([[26],{398:function(t,r,e){},455:function(t,r,e){"use strict";e(398)},484:function(t,r,e){"use strict";e.r(r);e(455);var a=e(33),s=Object(a.a)({},(function(){var t=this,r=t.$createElement,e=t._self._c||r;return e("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[e("h3",{attrs:{id:"_2022-02-09-새로-알게된-점"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#_2022-02-09-새로-알게된-점"}},[t._v("#")]),t._v(" 2022/02/09, 새로 알게된 점")]),t._v(" "),e("p",{staticClass:"tags"},[t._v("#transforme")]),t._v(" "),e("p",[t._v("  기다리던 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Transformer")])]),t._v(" 시간이다!")]),t._v(" "),e("h4",{attrs:{id:"transformer"}},[e("a",{staticClass:"header-anchor",attrs:{href:"#transformer"}},[t._v("#")]),t._v(" Transformer")]),t._v(" "),e("details",{staticClass:"custom-block details"},[e("summary",[t._v("Transformer Image")]),t._v(" "),e("p",[t._v("Transformer 관련한 모든 Image 는 "),e("a",{attrs:{href:"https://jalammar.github.io/illustrated-transformer/",target:"_blank",rel:"noopener noreferrer"}},[t._v("Jay Alammar GitHub"),e("OutboundLink")],1),t._v("속 Image 를 사용하였다.")])]),t._v(" "),e("p",[t._v("  Transformer 는 기본적으로 sequential data 를 다루는 방법론이다. 어제 배운 RNN, LSTM, GRU 과는 다른 방법론이지만 해결하려고 하는 문제는 비슷하다.\nsequential data 를 다루는 어려움에 대해서 다시 언급하자면,")]),t._v(" "),e("ul",[e("li",[t._v("Original sequence   ➜   sequential data 가 들어올 때,")]),t._v(" "),e("li",[t._v("Trimmed sequence    ➜   길이가 달라질 수도 있고,")]),t._v(" "),e("li",[t._v("Omitted sequence    ➜   중간 data 가 빠질 수도 있고,")]),t._v(" "),e("li",[t._v("Permuted sequence    ➜   data 가 밀리는? 경우도 있다.")])]),t._v(" "),e("p",[t._v("이러한 문제가 있어서, 위에 해당하는 data 를 이용하여 RNN 같이 sequential 하게 입력이 들어가는 모델링은 무척이나 어렵다.\n그래서 이러한 문제를 해결하고자 Transformer 를 개발했던 것 같고, "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("self-attention")])]),t._v(" 을 사용한다는 특징이 있다.")]),t._v(" "),e("p",[t._v("  Transformer 는 "),e("strong",[t._v('"Attention is All You Need"')]),t._v(", NIPS, 2017 에 GOOGLE 이 발표한 논문 속 모델이다.\nTransformer 는 기본적으로 sequential 한 데이터를 인코딩하는 방법이기 때문에, "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("NMT")])]),t._v("\n(Neural Machine Translation; 기계어 번역) 문제 뿐만 아니라, "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("classification")])]),t._v(",\n"),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("detection")])]),t._v(", ... 등 다양하게 적용된다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/01.png?raw=true"}}),t._v(" "),e("p",[t._v("  우리가 하려고 하는 것은, 문장이 주어지면 다른 언어로 바꾸는 것이다 (seq2seq).\n기존의 RNN을 사용하면 sequential data 길이만큼 재귀적으로 동작하지만, transformer 는 data 길이에 상관없이\n"),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("한 번에 입력")])]),t._v("을 받고, 인코딩 과정을 거친다. 물론 generation 할 때는 autoregressive 하게\n동작한다 (한 단어씩 만듬). 그리고 일반적으로 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("동일한 개수의 인코더와 디코더 레이어")])]),t._v("를 쌓는다.")]),t._v(" "),e("p",[t._v("  transformer 에서 반드시 알아야 하는 내용은 다음과 같다. "),e("strong",[t._v("첫 번째")]),t._v(", 어떻게 인코더에 한 번에 입력하는지를 알아야 하고,\n"),e("strong",[t._v("두 번째")]),t._v(", 마지막 인코더 레이어와 각각의 디코더들이 어떠한 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("정보")])]),t._v("를 주고받는지 알아야 하며,\n"),e("strong",[t._v("세 번째")]),t._v(", 마지막 디코더 레이어가 어떻게 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("generation")])]),t._v(" 할 수 있는지 알아야 한다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/02.png?raw=true"}}),t._v(" "),e("p",[t._v("  첫 번째로 입력 관련 내용이다. 한 번에 N 개의 sequential data 가 인코더 입력으로 들어간다는 것이다.\n하나의 인코더는 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Self-Attention")])]),t._v(" 과\n"),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Feed Forward Neural Network")])]),t._v(" 를 "),e("strong",[t._v("한 번씩")]),t._v(" 거치는 구조이다.\n그리고 출력되어 나오는 N 개의 값이 두 번째 인코더 레이어의 입력으로 들어간다.")]),t._v(" "),e("p",[t._v("중요한 점은 "),e("strong",[t._v("Self-Attention")]),t._v(" 이다. Transformer 의 성능이 좋도록 만드는 핵심이라는데, Attention 이 무엇이길래?")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/03.png?raw=true"}}),t._v(" "),e("p",[t._v("  예시를 들어보자, N 개의 단어가 들어간다고 했는데 예시에서는 3개의 단어가 들어간다고 가정해보자.\n그리고 기본적으로 기계가 번역할 수 있게 하기 위해서 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("단어마다 특정 숫자의 벡터로 표현(embedding)")])]),t._v("하게 된다.\n즉, 3개의 vector 가 입력으로 들어간다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/04.png?raw=true"}}),t._v(" "),e("p",[t._v("  그러면 인코더 레이어 속 Self-Attention 이 3개의 벡터를 3개의 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("feature vector")])]),t._v(" 로 만든다.\nN 개가 들어오면 N 개의 feature vector 로 만드는 것이다.\nvector ➜ vector 이 과정을 feed forward 로 볼 수도 있지만, 다른 중요한 점이라면 "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"-0.339ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"2.786ex",height:"1.885ex",viewBox:"0 -683 1231.6 833"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msub"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{attrs:{"data-c":"58",d:"M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"}})]),e("g",{attrs:{"data-mml-node":"mn",transform:"translate(828, -150) scale(0.707)"}},[e("path",{attrs:{"data-c":"31",d:"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"}})])])])])])]),t._v(" vector 가 "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"-0.339ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"2.458ex",height:"1.885ex",viewBox:"0 -683 1086.6 833"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msub"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{attrs:{"data-c":"5A",d:"M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"}})]),e("g",{attrs:{"data-mml-node":"mn",transform:"translate(683, -150) scale(0.707)"}},[e("path",{attrs:{"data-c":"31",d:"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"}})])])])])])]),t._v(" vector 로 될 때,\n단순히 "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"-0.339ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"2.786ex",height:"1.885ex",viewBox:"0 -683 1231.6 833"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msub"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{attrs:{"data-c":"58",d:"M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"}})]),e("g",{attrs:{"data-mml-node":"mn",transform:"translate(828, -150) scale(0.707)"}},[e("path",{attrs:{"data-c":"31",d:"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"}})])])])])])]),t._v(" vector 정보만 활용하는 것이 아니라 "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"-0.339ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"2.786ex",height:"1.885ex",viewBox:"0 -683 1231.6 833"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msub"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{attrs:{"data-c":"58",d:"M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"}})]),e("g",{attrs:{"data-mml-node":"mn",transform:"translate(828, -150) scale(0.707)"}},[e("path",{attrs:{"data-c":"32",d:"M109 429Q82 429 66 447T50 491Q50 562 103 614T235 666Q326 666 387 610T449 465Q449 422 429 383T381 315T301 241Q265 210 201 149L142 93L218 92Q375 92 385 97Q392 99 409 186V189H449V186Q448 183 436 95T421 3V0H50V19V31Q50 38 56 46T86 81Q115 113 136 137Q145 147 170 174T204 211T233 244T261 278T284 308T305 340T320 369T333 401T340 431T343 464Q343 527 309 573T212 619Q179 619 154 602T119 569T109 550Q109 549 114 549Q132 549 151 535T170 489Q170 464 154 447T109 429Z"}})])])])])])]),t._v(", "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"-0.375ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"2.786ex",height:"1.92ex",viewBox:"0 -683 1231.6 848.6"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msub"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{attrs:{"data-c":"58",d:"M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"}})]),e("g",{attrs:{"data-mml-node":"mn",transform:"translate(828, -150) scale(0.707)"}},[e("path",{attrs:{"data-c":"33",d:"M127 463Q100 463 85 480T69 524Q69 579 117 622T233 665Q268 665 277 664Q351 652 390 611T430 522Q430 470 396 421T302 350L299 348Q299 347 308 345T337 336T375 315Q457 262 457 175Q457 96 395 37T238 -22Q158 -22 100 21T42 130Q42 158 60 175T105 193Q133 193 151 175T169 130Q169 119 166 110T159 94T148 82T136 74T126 70T118 67L114 66Q165 21 238 21Q293 21 321 74Q338 107 338 175V195Q338 290 274 322Q259 328 213 329L171 330L168 332Q166 335 166 348Q166 366 174 366Q202 366 232 371Q266 376 294 413T322 525V533Q322 590 287 612Q265 626 240 626Q208 626 181 615T143 592T132 580H135Q138 579 143 578T153 573T165 566T175 555T183 540T186 520Q186 498 172 481T127 463Z"}})])])])])])]),t._v(" 정보까지 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("같이 활용")])]),t._v("한다.\n그래서 Self-Attention 은 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("dependency")])]),t._v(" 가 있다.")],1),t._v(" "),e("p",[t._v("그 다음에 만나는 feed forward 네트워크는 각각의 vector 에 대해서 dependency 가 없다. 단순히 통과시키며 변화시키는 것에 불과하다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/06.png?raw=true"}}),t._v(" "),e("p",[t._v('  영어 문장 "The animal didn\'t cross the street because it was too tired." 에서 it 의 의미를 찾는 과정을 Transformer 가 해준다.\n'),e("strong",[t._v("it")]),t._v(" 이라는 단어를 인코딩 할 때, 다른 단어들과의 관계성을 보게되고, 가장 관계가 높은 단어가 무엇인지 알아서 학습한다.\n그러다보니 단어를 더 잘 표현할 수 있다 (이해할 수 있다).")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/05.png?raw=true"}}),t._v(" "),e("p",[t._v("더 살펴보기 전에 문제를 단순히 하기 위해서, 3개의 단어가 아닌 2개의 단어가 입력되었다고 생각해보자.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/07.png?raw=true"}}),t._v(" "),e("p",[t._v("  Self-Attention 구조는 3가지 vector 를 만들어낸다. 3개의 vector 를 만들어내는 것은, 3개의 Neural Network 있다고 보면 된다.\n해당 vector 들은 각각 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Query")])]),t._v(", "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Key")])]),t._v(", 그리고\n"),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Value")])]),t._v(" 이다. 그래서 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("각각의 단어마다 Q, K, V vector")])]),t._v(" 가 있다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/08.png?raw=true"}}),t._v(" "),e("p",[t._v("  이 3개의 vector 를 통해서 우리가 "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"-0.339ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"2.207ex",height:"1.339ex",viewBox:"0 -442 975.6 592"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msub"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{attrs:{"data-c":"78",d:"M52 289Q59 331 106 386T222 442Q257 442 286 424T329 379Q371 442 430 442Q467 442 494 420T522 361Q522 332 508 314T481 292T458 288Q439 288 427 299T415 328Q415 374 465 391Q454 404 425 404Q412 404 406 402Q368 386 350 336Q290 115 290 78Q290 50 306 38T341 26Q378 26 414 59T463 140Q466 150 469 151T485 153H489Q504 153 504 145Q504 144 502 134Q486 77 440 33T333 -11Q263 -11 227 52Q186 -10 133 -10H127Q78 -10 57 16T35 71Q35 103 54 123T99 143Q142 143 142 101Q142 81 130 66T107 46T94 41L91 40Q91 39 97 36T113 29T132 26Q168 26 194 71Q203 87 217 139T245 247T261 313Q266 340 266 352Q266 380 251 392T217 404Q177 404 142 372T93 290Q91 281 88 280T72 278H58Q52 284 52 289Z"}})]),e("g",{attrs:{"data-mml-node":"mn",transform:"translate(572, -150) scale(0.707)"}},[e("path",{attrs:{"data-c":"31",d:"M213 578L200 573Q186 568 160 563T102 556H83V602H102Q149 604 189 617T245 641T273 663Q275 666 285 666Q294 666 302 660V361L303 61Q310 54 315 52T339 48T401 46H427V0H416Q395 3 257 3Q121 3 100 0H88V46H114Q136 46 152 46T177 47T193 50T201 52T207 57T213 61V578Z"}})])])])])])]),t._v(" 이라고 부르는 embedding vector 를 새로운 vector 로 바꿔줄 것이다.\n먼저 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Score vector")])]),t._v(" 를 만든다. Thinking 이라는 첫 번째 단어에 대한 Score vector 를 계산할 때,\n"),e("strong",[t._v("인코딩을 하고자 하는 vector")]),t._v(" 의 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Query vector")])]),t._v(" 와\n"),e("strong",[t._v("자기 자신을 포함한 모든 N 개의 vector")]),t._v(" 에 대한 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Key vector")])]),t._v(" 를 구한다.\n그리고 그 두 개의 vector 를 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("내적")])]),t._v(" (inner product) 한다.\n그러면 이 두 vector 가 얼마나 잘 align 되어 있는지 (나머지 단어들과 얼마나 유사도가 있는지, 관계가 있는지) 정한다.")],1),t._v(" "),e("p",[t._v("  내적한 것이 결국은 i 번째 vector 와 나머지 vector 사이에 얼마나 interact 해야하는지 알아서 학습하게한다. 이것이 Attention 이다.\n즉, 특정 task 를 수행할 때 어떤 입력을 주의 깊게 볼지 정하는 것이다. 그 다음 과정은 단순하다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/09.png?raw=true"}}),t._v(" "),e("p",[t._v("  Score vector 가 나오면 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("normalize")])]),t._v(" 한다. Normalize 시 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("8")])]),t._v("로\n나눠주는데, 이 숫자는 "),e("strong",[t._v("Key vector 의 차원")]),t._v("에 연관이 있다. 우리가 Key vector 의 차원을 얼만큼 할지 "),e("strong",[t._v("하이퍼파라미터")]),t._v("로 정하는 것이다.\n위의 예시에서는 64개의 vector 로 만들었고, "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"-0.339ex"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"4.147ex",height:"2.398ex",viewBox:"0 -910 1833 1060"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"msqrt"}},[e("g",{attrs:{transform:"translate(833, 0)"}},[e("g",{attrs:{"data-mml-node":"mn"}},[e("path",{attrs:{"data-c":"36",d:"M42 313Q42 476 123 571T303 666Q372 666 402 630T432 550Q432 525 418 510T379 495Q356 495 341 509T326 548Q326 592 373 601Q351 623 311 626Q240 626 194 566Q147 500 147 364L148 360Q153 366 156 373Q197 433 263 433H267Q313 433 348 414Q372 400 396 374T435 317Q456 268 456 210V192Q456 169 451 149Q440 90 387 34T253 -22Q225 -22 199 -14T143 16T92 75T56 172T42 313ZM257 397Q227 397 205 380T171 335T154 278T148 216Q148 133 160 97T198 39Q222 21 251 21Q302 21 329 59Q342 77 347 104T352 209Q352 289 347 316T329 361Q302 397 257 397Z"}}),e("path",{attrs:{"data-c":"34",d:"M462 0Q444 3 333 3Q217 3 199 0H190V46H221Q241 46 248 46T265 48T279 53T286 61Q287 63 287 115V165H28V211L179 442Q332 674 334 675Q336 677 355 677H373L379 671V211H471V165H379V114Q379 73 379 66T385 54Q393 47 442 46H471V0H462ZM293 211V545L74 212L183 211H293Z",transform:"translate(500, 0)"}})])]),e("g",{attrs:{"data-mml-node":"mo",transform:"translate(0, 50)"}},[e("path",{attrs:{"data-c":"221A",d:"M95 178Q89 178 81 186T72 200T103 230T169 280T207 309Q209 311 212 311H213Q219 311 227 294T281 177Q300 134 312 108L397 -77Q398 -77 501 136T707 565T814 786Q820 800 834 800Q841 800 846 794T853 782V776L620 293L385 -193Q381 -200 366 -200Q357 -200 354 -197Q352 -195 256 15L160 225L144 214Q129 202 113 190T95 178Z"}})]),e("rect",{attrs:{width:"1000",height:"60",x:"833",y:"790"}})])])])])]),t._v(" 를 통해 나온 숫자이다. Score 값이 적정 범위를 유지하도록 하는 것이다.")],1),t._v(" "),e("p",[t._v("  결과적으로, "),e("u",[e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Key vector 의 차원과 Query vector 의 차원이 같아")])])]),t._v("야 내적을 할 수 있고,\n내적 값을 Key vector 의 차원으로 나눠 normalize 한다.\n그리고 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("softmax")])]),t._v(" 를 취한다. 결과 값 0.88 의 의미는, Thnking 단어가 자기 자신과 attention 에 대한의 interation 의\n값이라는 것이다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/10.png?raw=true"}}),t._v(" "),e("p",[t._v("  각각의 단어의 Score 는 scala 값인데, 이를 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Value vector")])]),t._v(" 에 곱함으로써 weights 에 "),e("strong",[t._v("가중치")]),t._v("를 더한다.\nValue vector 는 차원이 달라도 된다 ( "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("하지만 편의상 같게 만든다")])]),t._v(" ).\n그리고 최종적으로 나오는 Thinking 이라는 인코딩된 vector 의 차원은 "),e("strong",[t._v("value vector 의 차원과 동일")]),t._v("하다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/11.png?raw=true"}}),t._v(" "),e("p",[t._v("  말로 설명해서 복잡하지만, 행렬을 이용하여 계산하는 과정은 간단하다."),e("br"),t._v("\n위 그림에서 "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"0"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"1.873ex",height:"1.545ex",viewBox:"0 -683 828 683"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{attrs:{"data-c":"58",d:"M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"}})])])])])]),t._v(" 의 shape 이 (2, 4) 인데, 2는 "),e("strong",[t._v("단어의 개수")]),t._v("를 의미하고, 각 단어마다 4차원으로 표현한다는 의미이다.\n그리고 3개의 Weight (about Query, Key, Value) 와 "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"0"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"1.873ex",height:"1.545ex",viewBox:"0 -683 828 683"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{attrs:{"data-c":"58",d:"M42 0H40Q26 0 26 11Q26 15 29 27Q33 41 36 43T55 46Q141 49 190 98Q200 108 306 224T411 342Q302 620 297 625Q288 636 234 637H206Q200 643 200 645T202 664Q206 677 212 683H226Q260 681 347 681Q380 681 408 681T453 682T473 682Q490 682 490 671Q490 670 488 658Q484 643 481 640T465 637Q434 634 411 620L488 426L541 485Q646 598 646 610Q646 628 622 635Q617 635 609 637Q594 637 594 648Q594 650 596 664Q600 677 606 683H618Q619 683 643 683T697 681T738 680Q828 680 837 683H845Q852 676 852 672Q850 647 840 637H824Q790 636 763 628T722 611T698 593L687 584Q687 585 592 480L505 384Q505 383 536 304T601 142T638 56Q648 47 699 46Q734 46 734 37Q734 35 732 23Q728 7 725 4T711 1Q708 1 678 1T589 2Q528 2 496 2T461 1Q444 1 444 10Q444 11 446 25Q448 35 450 39T455 44T464 46T480 47T506 54Q523 62 523 64Q522 64 476 181L429 299Q241 95 236 84Q232 76 232 72Q232 53 261 47Q262 47 267 47T273 46Q276 46 277 46T280 45T283 42T284 35Q284 26 282 19Q279 6 276 4T261 1Q258 1 243 1T201 2T142 2Q64 2 42 0Z"}})])])])])]),t._v(" 와의 곱으로 Q, K, V vector 를 구하는데, 단어가 2개이므로 "),e("code",[t._v("shape[0]=2")]),t._v(" 가 된다.")],1),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/12.png?raw=true"}}),t._v(" "),e("p",[t._v("  그런 다음 Query vector 와 Key vector 를 내적해서 나온 값을 Key vector 차원으로 normalize 한 뒤 softmax 를 취하고,\n해당 scalar 값을 Value vector 에 weight sum 하면 Self-Attention 부분은 끝난다. TensorFlow 나 PyTorch 에서는 1~2 줄로 구현할 수 있다.")]),t._v(" "),e("p",[t._v("  그러면 왜 이러한 방식이 잘 될까?"),e("br"),t._v("\nInput 형태가 고정된 기존 모델과는 달리, flexible 하게 input 을 받을 수 있고, 이로 인해 더 많은 것을 표현할 수 있어서이지 않나 싶다.\n다만 Input 인 embedding vector 길이가 길어질 수록 "),e("strong",[t._v("메모리 부담")]),t._v("이 커지게 된다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/13.png?raw=true"}}),t._v(" "),e("p",[t._v("  이번에는 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("MHA")])]),t._v(" (Multi-headed attention) 이다."),e("br"),t._v("\n별건 없고, 위에서 길게 설명한 Attention 과정을 "),e("strong",[t._v("여러번 하는 것")]),t._v("이다. 한 개의 embedding vector 에 대해서 여러개의 Query, Key, Value vector 를 만든다고 보면 된다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/14.png?raw=true"}}),t._v(" "),e("p",[t._v("  이를 통해서 N 개의 인코딩된 vector 를 얻게 된다 (위 예시에서는 8개). 다만 문제라면, 하나의 인코더 레이어를 통해 나온 인코딩 벡터가\n다음 인코더 레이어로 들어가기 때문에, "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("입력 - 출력 차원을 맞춰줄 필요")])]),t._v("가 있다.\n즉, embedding vector 와 인코딩 vector 의 차원이 같아야 한다.")]),t._v(" "),e("p",[t._v("  MHA 를 통해 N 개의 인코딩 vector 가 나왔으니까 linear 하게 이어붙이면,\n차원은 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("( 단어의 개수, embedding 차원 x MHA 횟수(N) )")])]),t._v(" 이다.\n이를 embedding vector 의 차원으로 축소시키려면 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("( embedding 차원 x MHA 횟수(N), embedding 차원 )")])]),t._v(" 크기로 곱하면 된다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/15.png?raw=true"}}),t._v(" "),e("p",[t._v("전반적인 과정은 위의 그림과 같다. 하지만 실제 코드를 이렇게 구현하지 않았다.\nMHA 를 linear 하게 합치는 것이 아니라, "),e("u",[e("strong",[t._v("embedding vector 의 차원을 MHA 개수로 나눈 것으로 진행")])]),t._v("한다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/16.png?raw=true"}}),t._v(" "),e("p",[t._v("  한 가지 빠진 부분이 있다. Self-Attention 에 embedding vector 가 들어가기 전에,\n"),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Positional Encoding")])]),t._v(" 이 추가된다. 입력에 순저 정보를 더해주는 것으로써,\n"),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("bias")])]),t._v(" 라고 생각하면 된다.\n이전의 과정을 돌이켜보면, Attention 으로 단어와의 관계를 학습했지만 그 속에 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("순서 정보는 없다")])]),t._v(".\n그래서 순서에 대한 정보를 더하기 위해 positional encoding 을 더한다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/17.png?raw=true"}}),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/18.png?raw=true"}}),t._v(" "),e("p",[t._v("Positional Encoding 은 특정 방법으로 만들게 된다. 우리가 살펴본 예시의 경우 embedding vector 가 4차원인 경우이고,\n논문에서는 512차원으로 진행했다. 그리고 최근에는 두 번째 그림과 같이 적용한다고 한다.\n이후의 과정은 다른 모델들과 크게 다를 바가 없다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/19.png?raw=true"}}),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/20.png?raw=true"}}),t._v(" "),e("p",[t._v("Self-Attetion 으로 N 개의 단어가 주어지면, N 개의 인코딩 vector "),e("mjx-container",{staticClass:"MathJax",attrs:{jax:"SVG"}},[e("svg",{staticStyle:{"vertical-align":"0"},attrs:{xmlns:"http://www.w3.org/2000/svg",width:"1.545ex",height:"1.545ex",viewBox:"0 -683 683 683"}},[e("g",{attrs:{stroke:"currentColor",fill:"currentColor","stroke-width":"0",transform:"matrix(1 0 0 -1 0 0)"}},[e("g",{attrs:{"data-mml-node":"math"}},[e("g",{attrs:{"data-mml-node":"mi"}},[e("path",{attrs:{"data-c":"5A",d:"M58 8Q58 23 64 35Q64 36 329 334T596 635L586 637Q575 637 512 637H500H476Q442 637 420 635T365 624T311 598T266 548T228 469Q227 466 226 463T224 458T223 453T222 450L221 448Q218 443 202 443Q185 443 182 453L214 561Q228 606 241 651Q249 679 253 681Q256 683 487 683H718Q723 678 723 675Q723 673 717 649Q189 54 188 52L185 49H274Q369 50 377 51Q452 60 500 100T579 247Q587 272 590 277T603 282H607Q628 282 628 271Q547 5 541 2Q538 0 300 0H124Q58 0 58 8Z"}})])])])])]),t._v(" 가 나오고 Layer Normalization 과정을 거친다.")],1),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/21.png?raw=true"}}),t._v(" "),e("p",[t._v("  그런 다음 Feed Forward 를 거친다. 이 과정이 반복된다."),e("br"),t._v("\n인코딩 과정은 단어를 표현하는 것이었고 이를 통해 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("디코더가 생성")])]),t._v("해야 한다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/22.gif?raw=true"}}),t._v(" "),e("p",[t._v("  그래서 두 번째로 디코더가 받는 정보 부분이다."),e("br"),t._v("\n마지막 인코더는 각각의 디코더에 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Key")])]),t._v(" 와 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Value")])]),t._v(" 를 보낸다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/23.gif?raw=true"}}),t._v(" "),e("p",[t._v("  최종 출력은 autoregressive 하게 한 단어씩 만든다."),e("br"),t._v("\n그런데 i 번째 단어를 만들 때, 모든 문장을 다 알고 있으면 학습 의미가 없으므로, 학습 단계에서 "),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Masking")])]),t._v(" 을 하게 된다.\nMasking 의 의미는, 이전 단어들에 대해서만 연관이 있고, 뒤에 있는 단어들에 대해서는 독립적으로 만드는 것을 말한다.")]),t._v(" "),e("br"),t._v(" "),e("img",{attrs:{src:"https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/24.png?raw=true"}}),t._v(" "),e("p",[t._v("마지막 layer 에서 단어들의 분포를 만들어서, 매번 그 중에서 단어를 샘플링하는 방식으로 동작하게 된다.")]),t._v(" "),e("br"),t._v(" "),e("p",[t._v("여기까지 알아본 Transformer 는 사실 NMT 문제에서만 사용되었었는데, 점차 시간이 지나며 단어들의 sequence 를 다루는 것 뿐만이 아니라,\n"),e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("이미지에서도 활용")])]),t._v("하고있다.")]),t._v(" "),e("p",[e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("Vision Transformer")])]),t._v("   ➜   "),e("a",{attrs:{href:"https://arxiv.org/abs/2010.11929",target:"_blank",rel:"noopener noreferrer"}},[e("strong",[t._v("논문")]),e("OutboundLink")],1)]),t._v(" "),e("p",[t._v("이미지 분류를 할 때 인코더만 활용한다. 그리고 인코더에서 나오는 첫 번째 encoded vector 를 classifier 에 입력하는 구조이다.\nNMT 에서는 문장이 입력되었다면, 이미지에서는 일정 구간으로 나눠 이를 입력시켰다. 마찬가지로 positional encoding 이 들어간다.")]),t._v(" "),e("br"),t._v(" "),e("p",[t._v("그리고 문장이 주어지면 문장에 대한 이미지를 만들어내는 "),e("strong",[t._v("DALL-E")]),t._v(" 도 있다. openai 에서 말하길, transformer 에서 "),e("strong",[t._v("디코더")]),t._v("만 활용했고,\n이미지를 16x16 ? grid 로 나눠 sequencial 하게 입력하고 문장도 입력해서, 새로운 문장이 주어졌을 때 그 문장에 대한 이미지를 만든다고 한다.")]),t._v(" "),e("p",[e("span",{staticStyle:{color:"#2454ff"}},[e("strong",[t._v("DALL-E")])]),t._v("   ➜   "),e("a",{attrs:{href:"https://openai.com/blog/dall-e",target:"_blank",rel:"noopener noreferrer"}},[e("strong",[t._v("openai")]),e("OutboundLink")],1)]),t._v(" "),e("br")])}),[],!1,null,"a5e7c8d4",null);r.default=s.exports}}]);