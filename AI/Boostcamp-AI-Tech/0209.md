---
title: 02/09 Summary
lang: ko-KR

meta:
  - name: description
    content: 부스트캠프 AI Tech 02/09 Summary
  - property: og:title
    content: 02/09 Summary
  - property: og:description
    content: 부스트캠프 AI Tech 02/09 Summary
  - property: og:image
    content: 'https://boostcamp.connect.or.kr/images/pavicon_180_v2.ico'
  - property: og:url
    content: https://leedooho.com/AI/Boostcamp-AI-Tech/0209.html
---

### 2022/02/09, 새로 알게된 점

<p class="tags">#Transformer</p>

&nbsp; 기다리던 <span style="color: #2454ff;">**Transformer**</span> 시간이다!

#### Transformer

::: details Transformer Image
Transformer 관련한 모든 Image 는 [Jay Alammar GitHub](https://jalammar.github.io/illustrated-transformer/)속 Image 를 사용하였다.
:::

&nbsp; Transformer 는 기본적으로 sequential data 를 다루는 방법론이다. 어제 배운 RNN, LSTM, GRU 과는 다른 방법론이지만 해결하려고 하는 문제는 비슷하다. 
sequential data 를 다루는 어려움에 대해서 다시 언급하자면, 

- Original sequence &nbsp; &#10140; &nbsp; sequential data 가 들어올 때,
- Trimmed sequence  &nbsp; &#10140; &nbsp; 길이가 달라질 수도 있고,
- Omitted sequence  &nbsp; &#10140; &nbsp; 중간 data 가 빠질 수도 있고,
- Permuted sequence  &nbsp; &#10140; &nbsp; data 가 밀리는? 경우도 있다.

이러한 문제가 있어서, 위에 해당하는 data 를 이용하여 RNN 같이 sequential 하게 입력이 들어가는 모델링은 무척이나 어렵다. 
그래서 이러한 문제를 해결하고자 Transformer 를 개발했던 것 같고, <span style="color: #2454ff;">**self-attention**</span> 을 사용한다는 특징이 있다. 

&nbsp; Transformer 는 **"Attention is All You Need"**, NIPS, 2017 에 GOOGLE 이 발표한 논문 속 모델이다. 
Transformer 는 기본적으로 sequential 한 데이터를 인코딩하는 방법이기 때문에, <span style="color: #2454ff;">**NMT**</span> 
(Neural Machine Translation; 기계어 번역) 문제 뿐만 아니라, <span style="color: #2454ff;">**classification**</span>, 
<span style="color: #2454ff;">**detection**</span>, ... 등 다양하게 적용된다. 

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/01.png?raw=true">

&nbsp; 우리가 하려고 하는 것은, 문장이 주어지면 다른 언어로 바꾸는 것이다 (seq2seq). 
기존의 RNN을 사용하면 sequential data 길이만큼 재귀적으로 동작하지만, transformer 는 data 길이에 상관없이 
<span style="color: #2454ff;">**한 번에 입력**</span>을 받고, 인코딩 과정을 거친다. 물론 generation 할 때는 autoregressive 하게 
동작한다 (한 단어씩 만듬). 그리고 일반적으로 <span style="color: #2454ff;">**동일한 개수의 인코더와 디코더 레이어**</span>를 쌓는다. 

&nbsp; transformer 에서 반드시 알아야 하는 내용은 다음과 같다. **첫 번째**, 어떻게 인코더에 한 번에 입력하는지를 알아야 하고, 
**두 번째**, 마지막 인코더 레이어와 각각의 디코더들이 어떠한 <span style="color: #2454ff;">**정보**</span>를 주고받는지 알아야 하며, 
**세 번째**, 마지막 디코더 레이어가 어떻게 <span style="color: #2454ff;">**generation**</span> 할 수 있는지 알아야 한다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/02.png?raw=true">

&nbsp; 첫 번째로 입력 관련 내용이다. 한 번에 N 개의 sequential data 가 인코더 입력으로 들어간다는 것이다. 
하나의 인코더는 <span style="color: #2454ff;">**Self-Attention**</span> 과 
<span style="color: #2454ff;">**Feed Forward Neural Network**</span> 를 **한 번씩** 거치는 구조이다. 
그리고 출력되어 나오는 N 개의 값이 두 번째 인코더 레이어의 입력으로 들어간다. 

중요한 점은 **Self-Attention** 이다. Transformer 의 성능이 좋도록 만드는 핵심이라는데, Attention 이 무엇이길래?

<br>

<img srf="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/03.png?raw=true">

&nbsp; 예시를 들어보자, N 개의 단어가 들어간다고 했는데 예시에서는 3개의 단어가 들어간다고 가정해보자. 
그리고 기본적으로 기계가 번역할 수 있게 하기 위해서 <span style="color: #2454ff;">**단어마다 특정 숫자의 벡터로 표현(embedding)**</span>하게 된다. 
즉, 3개의 vector 가 입력으로 들어간다.

<br>

<img srf="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/04.png?raw=true">

&nbsp; 그러면 인코더 레이어 속 Self-Attention 이 3개의 벡터를 3개의 <span style="color: #2454ff;">**feature vector**</span> 로 만든다. 
N 개가 들어오면 N 개의 feature vector 로 만드는 것이다.
vector &#10140; vector 이 과정을 feed forward 로 볼 수도 있지만, 다른 중요한 점이라면 $X_1$ vector 가 $Z_1$ vector 로 될 때, 
단순히 $X_1$ vector 정보만 활용하는 것이 아니라 $X_2$, $X_3$ 정보까지 <span style="color: #2454ff;">**같이 활용**</span>한다. 
그래서 Self-Attention 은 <span style="color: #2454ff;">**dependency**</span> 가 있다. 

그 다음에 만나는 feed forward 네트워크는 각각의 vector 에 대해서 dependency 가 없다. 단순히 통과시키며 변화시키는 것에 불과하다. 

<br>

<img srf="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/06.png?raw=true">

&nbsp; 영어 문장 "The animal didn't cross the street because it was too tired." 에서 it 의 의미를 찾는 과정을 Transformer 가 해준다. 
**it** 이라는 단어를 인코딩 할 때, 다른 단어들과의 관계성을 보게되고, 가장 관계가 높은 단어가 무엇인지 알아서 학습한다. 
그러다보니 단어를 더 잘 표현할 수 있다 (이해할 수 있다).

<br>

<img srf="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/05.png?raw=true">

더 살펴보기 전에 문제를 단순히 하기 위해서, 3개의 단어가 아닌 2개의 단어가 입력되었다고 생각해보자. 

<br>

<img srf="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/07.png?raw=true">

&nbsp; 

<br>

<hr>

### 피어 세션

<br>

<br>

<br>

<style scoped>
.tags { color: #2454ff; }
a { color: #2454ff; }
</style>