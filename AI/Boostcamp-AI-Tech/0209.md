---
title: 02/09 Summary
lang: ko-KR

meta:
  - name: description
    content: 부스트캠프 AI Tech 02/09 Summary
  - property: og:title
    content: 02/09 Summary
  - property: og:description
    content: 부스트캠프 AI Tech 02/09 Summary
  - property: og:image
    content: 'https://boostcamp.connect.or.kr/images/pavicon_180_v2.ico'
  - property: og:url
    content: https://leedooho.com/AI/Boostcamp-AI-Tech/0209.html
---

### 2022/02/09, 새로 알게된 점

<p class="tags">#transformer #generative_model</p>

&nbsp; 기다리던 <span style="color: #2454ff;">**Transformer**</span> 시간이다!

#### Transformer

::: details Transformer Image
Transformer 관련한 모든 Image 는 [Jay Alammar GitHub](https://jalammar.github.io/illustrated-transformer/)속 Image 를 사용하였다.
:::

&nbsp; Transformer 는 기본적으로 sequential data 를 다루는 방법론이다. 어제 배운 RNN, LSTM, GRU 과는 다른 방법론이지만 해결하려고 하는 문제는 비슷하다. 
sequential data 를 다루는 어려움에 대해서 다시 언급하자면, 

- Original sequence &nbsp; &#10140; &nbsp; sequential data 가 들어올 때,
- Trimmed sequence  &nbsp; &#10140; &nbsp; 길이가 달라질 수도 있고,
- Omitted sequence  &nbsp; &#10140; &nbsp; 중간 data 가 빠질 수도 있고,
- Permuted sequence  &nbsp; &#10140; &nbsp; data 가 밀리는? 경우도 있다.

이러한 문제가 있어서, 위에 해당하는 data 를 이용하여 RNN 같이 sequential 하게 입력이 들어가는 모델링은 무척이나 어렵다. 
그래서 이러한 문제를 해결하고자 Transformer 를 개발했던 것 같고, <span style="color: #2454ff;">**self-attention**</span> 을 사용한다는 특징이 있다. 

&nbsp; Transformer 는 **"Attention is All You Need"**, NIPS, 2017 에 GOOGLE 이 발표한 논문 속 모델이다. 
Transformer 는 기본적으로 sequential 한 데이터를 인코딩하는 방법이기 때문에, <span style="color: #2454ff;">**NMT**</span> 
(Neural Machine Translation; 기계어 번역) 문제 뿐만 아니라, <span style="color: #2454ff;">**classification**</span>, 
<span style="color: #2454ff;">**detection**</span>, ... 등 다양하게 적용된다. 

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/01.png?raw=true">

&nbsp; 우리가 하려고 하는 것은, 문장이 주어지면 다른 언어로 바꾸는 것이다 (seq2seq). 
기존의 RNN을 사용하면 sequential data 길이만큼 재귀적으로 동작하지만, transformer 는 data 길이에 상관없이 
<span style="color: #2454ff;">**한 번에 입력**</span>을 받고, 인코딩 과정을 거친다. 물론 generation 할 때는 autoregressive 하게 
동작한다 (한 단어씩 만듬). 그리고 일반적으로 <span style="color: #2454ff;">**동일한 개수의 인코더와 디코더 레이어**</span>를 쌓는다. 

&nbsp; transformer 에서 반드시 알아야 하는 내용은 다음과 같다. **첫 번째**, 어떻게 인코더에 한 번에 입력하는지를 알아야 하고, 
**두 번째**, 마지막 인코더 레이어와 각각의 디코더들이 어떠한 <span style="color: #2454ff;">**정보**</span>를 주고받는지 알아야 하며, 
**세 번째**, 마지막 디코더 레이어가 어떻게 <span style="color: #2454ff;">**generation**</span> 할 수 있는지 알아야 한다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/02.png?raw=true">

&nbsp; 첫 번째로 입력 관련 내용이다. 한 번에 N 개의 sequential data 가 인코더 입력으로 들어간다는 것이다. 
하나의 인코더는 <span style="color: #2454ff;">**Self-Attention**</span> 과 
<span style="color: #2454ff;">**Feed Forward Neural Network**</span> 를 **한 번씩** 거치는 구조이다. 
그리고 출력되어 나오는 N 개의 값이 두 번째 인코더 레이어의 입력으로 들어간다. 

중요한 점은 **Self-Attention** 이다. Transformer 의 성능이 좋도록 만드는 핵심이라는데, Attention 이 무엇이길래?

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/03.png?raw=true">

&nbsp; 예시를 들어보자, N 개의 단어가 들어간다고 했는데 예시에서는 3개의 단어가 들어간다고 가정해보자. 
그리고 기본적으로 기계가 번역할 수 있게 하기 위해서 <span style="color: #2454ff;">**단어마다 특정 숫자의 벡터로 표현(embedding)**</span>하게 된다. 
즉, 3개의 vector 가 입력으로 들어간다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/04.png?raw=true">

&nbsp; 그러면 인코더 레이어 속 Self-Attention 이 3개의 벡터를 3개의 <span style="color: #2454ff;">**feature vector**</span> 로 만든다. 
N 개가 들어오면 N 개의 feature vector 로 만드는 것이다.
vector &#10140; vector 이 과정을 feed forward 로 볼 수도 있지만, 다른 중요한 점이라면 $X_1$ vector 가 $Z_1$ vector 로 될 때, 
단순히 $X_1$ vector 정보만 활용하는 것이 아니라 $X_2$, $X_3$ 정보까지 <span style="color: #2454ff;">**같이 활용**</span>한다. 
그래서 Self-Attention 은 <span style="color: #2454ff;">**dependency**</span> 가 있다. 

그 다음에 만나는 feed forward 네트워크는 각각의 vector 에 대해서 dependency 가 없다. 단순히 통과시키며 변화시키는 것에 불과하다. 

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/06.png?raw=true">

&nbsp; 영어 문장 "The animal didn't cross the street because it was too tired." 에서 it 의 의미를 찾는 과정을 Transformer 가 해준다. 
**it** 이라는 단어를 인코딩 할 때, 다른 단어들과의 관계성을 보게되고, 가장 관계가 높은 단어가 무엇인지 알아서 학습한다. 
그러다보니 단어를 더 잘 표현할 수 있다 (이해할 수 있다).

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/05.png?raw=true">

더 살펴보기 전에 문제를 단순히 하기 위해서, 3개의 단어가 아닌 2개의 단어가 입력되었다고 생각해보자. 

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/07.png?raw=true">

&nbsp; Self-Attention 구조는 3가지 vector 를 만들어낸다. 3개의 vector 를 만들어내는 것은, 3개의 Neural Network 있다고 보면 된다. 
해당 vector 들은 각각 <span style="color: #2454ff;">**Query**</span>, <span style="color: #2454ff;">**Key**</span>, 그리고 
<span style="color: #2454ff;">**Value**</span> 이다. 그래서 <span style="color: #2454ff;">**각각의 단어마다 Q, K, V vector**</span> 가 있다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/08.png?raw=true">

&nbsp; 이 3개의 vector 를 통해서 우리가 $x_1$ 이라고 부르는 embedding vector 를 새로운 vector 로 바꿔줄 것이다. 
먼저 <span style="color: #2454ff;">**Score vector**</span> 를 만든다. Thinking 이라는 첫 번째 단어에 대한 Score vector 를 계산할 때, 
**인코딩을 하고자 하는 vector** 의 <span style="color: #2454ff;">**Query vector**</span> 와 
**자기 자신을 포함한 모든 N 개의 vector** 에 대한 <span style="color: #2454ff;">**Key vector**</span> 를 구한다. 
그리고 그 두 개의 vector 를 <span style="color: #2454ff;">**내적**</span> (inner product) 한다. 
그러면 이 두 vector 가 얼마나 잘 align 되어 있는지 (나머지 단어들과 얼마나 유사도가 있는지, 관계가 있는지) 정한다.

&nbsp; 내적한 것이 결국은 i 번째 vector 와 나머지 vector 사이에 얼마나 interact 해야하는지 알아서 학습하게한다. 이것이 Attention 이다. 
즉, 특정 task 를 수행할 때 어떤 입력을 주의 깊게 볼지 정하는 것이다. 그 다음 과정은 단순하다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/09.png?raw=true">

&nbsp; Score vector 가 나오면 <span style="color: #2454ff;">**normalize**</span> 한다. Normalize 시 <span style="color: #2454ff;">**8**</span>로 
나눠주는데, 이 숫자는 **Key vector 의 차원**에 연관이 있다. 우리가 Key vector 의 차원을 얼만큼 할지 **하이퍼파라미터**로 정하는 것이다. 
위의 예시에서는 64개의 vector 로 만들었고, $\sqrt{64}$ 를 통해 나온 숫자이다. Score 값이 적정 범위를 유지하도록 하는 것이다. 

&nbsp; 결과적으로, <u><span style="color: #2454ff;">**Key vector 의 차원과 Query vector 의 차원이 같아**</span></u>야 내적을 할 수 있고, 
내적 값을 Key vector 의 차원으로 나눠 normalize 한다. 
그리고 <span style="color: #2454ff;">**softmax**</span> 를 취한다. 결과 값 0.88 의 의미는, Thnking 단어가 자기 자신과 attention 에 대한의 interation 의 
값이라는 것이다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/10.png?raw=true">

&nbsp; 각각의 단어의 Score 는 scala 값인데, 이를 <span style="color: #2454ff;">**Value vector**</span> 에 곱함으로써 weights 에 **가중치**를 더한다. 
Value vector 는 차원이 달라도 된다 ( <span style="color: #2454ff;">**하지만 편의상 같게 만든다**</span> ). 
그리고 최종적으로 나오는 Thinking 이라는 인코딩된 vector 의 차원은 **value vector 의 차원과 동일**하다. 

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/11.png?raw=true">

&nbsp; 말로 설명해서 복잡하지만, 행렬을 이용하여 계산하는 과정은 간단하다.<br>
위 그림에서 $X$ 의 shape 이 (2, 4) 인데, 2는 **단어의 개수**를 의미하고, 각 단어마다 4차원으로 표현한다는 의미이다. 
그리고 3개의 Weight (about Query, Key, Value) 와 $X$ 와의 곱으로 Q, K, V vector 를 구하는데, 단어가 2개이므로 `shape[0]=2` 가 된다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/12.png?raw=true">

&nbsp; 그런 다음 Query vector 와 Key vector 를 내적해서 나온 값을 Key vector 차원으로 normalize 한 뒤 softmax 를 취하고, 
해당 scalar 값을 Value vector 에 weight sum 하면 Self-Attention 부분은 끝난다. TensorFlow 나 PyTorch 에서는 1~2 줄로 구현할 수 있다.

&nbsp; 그러면 왜 이러한 방식이 잘 될까?<br>
Input 형태가 고정된 기존 모델과는 달리, flexible 하게 input 을 받을 수 있고, 이로 인해 더 많은 것을 표현할 수 있어서이지 않나 싶다. 
다만 Input 인 embedding vector 길이가 길어질 수록 **메모리 부담**이 커지게 된다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/13.png?raw=true">

&nbsp; 이번에는 <span style="color: #2454ff;">**MHA**</span> (Multi-headed attention) 이다.<br>
별건 없고, 위에서 길게 설명한 Attention 과정을 **여러번 하는 것**이다. 한 개의 embedding vector 에 대해서 여러개의 Query, Key, Value vector 를 만든다고 보면 된다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/14.png?raw=true">

&nbsp; 이를 통해서 N 개의 인코딩된 vector 를 얻게 된다 (위 예시에서는 8개). 다만 문제라면, 하나의 인코더 레이어를 통해 나온 인코딩 벡터가 
다음 인코더 레이어로 들어가기 때문에, <span style="color: #2454ff;">**입력 - 출력 차원을 맞춰줄 필요**</span>가 있다. 
즉, embedding vector 와 인코딩 vector 의 차원이 같아야 한다.

&nbsp; MHA 를 통해 N 개의 인코딩 vector 가 나왔으니까 linear 하게 이어붙이면, 
차원은 <span style="color: #2454ff;">**( 단어의 개수, embedding 차원 x MHA 횟수(N) )**</span> 이다. 
이를 embedding vector 의 차원으로 축소시키려면 <span style="color: #2454ff;">**( embedding 차원 x MHA 횟수(N), embedding 차원 )**</span> 크기로 곱하면 된다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/15.png?raw=true">

전반적인 과정은 위의 그림과 같다. 하지만 실제 코드를 이렇게 구현하지 않았다. 
MHA 를 linear 하게 합치는 것이 아니라, <u>**embedding vector 의 차원을 MHA 개수로 나눈 것으로 진행**</u>한다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/16.png?raw=true">

&nbsp; 한 가지 빠진 부분이 있다. Self-Attention 에 embedding vector 가 들어가기 전에, 
<span style="color: #2454ff;">**Positional Encoding**</span> 이 추가된다. 입력에 순저 정보를 더해주는 것으로써, 
<span style="color: #2454ff;">**bias**</span> 라고 생각하면 된다. 
이전의 과정을 돌이켜보면, Attention 으로 단어와의 관계를 학습했지만 그 속에 <span style="color: #2454ff;">**순서 정보는 없다**</span>. 
그래서 순서에 대한 정보를 더하기 위해 positional encoding 을 더한다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/17.png?raw=true">
<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/18.png?raw=true">

Positional Encoding 은 특정 방법으로 만들게 된다. 우리가 살펴본 예시의 경우 embedding vector 가 4차원인 경우이고, 
논문에서는 512차원으로 진행했다. 그리고 최근에는 두 번째 그림과 같이 적용한다고 한다. 
이후의 과정은 다른 모델들과 크게 다를 바가 없다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/19.png?raw=true">
<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/20.png?raw=true">

Self-Attetion 으로 N 개의 단어가 주어지면, N 개의 인코딩 vector $Z$ 가 나오고 Layer Normalization 과정을 거친다. 

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/21.png?raw=true">

&nbsp; 그런 다음 Feed Forward 를 거친다. 이 과정이 반복된다.<br>
인코딩 과정은 단어를 표현하는 것이었고 이를 통해 <span style="color: #2454ff;">**디코더가 생성**</span>해야 한다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/22.gif?raw=true">

&nbsp; 그래서 두 번째로 디코더가 받는 정보 부분이다.<br>
마지막 인코더는 각각의 디코더에 <span style="color: #2454ff;">**Key**</span> 와 <span style="color: #2454ff;">**Value**</span> 를 보낸다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/23.gif?raw=true">

&nbsp; 최종 출력은 autoregressive 하게 한 단어씩 만든다.<br>
그런데 i 번째 단어를 만들 때, 모든 문장을 다 알고 있으면 학습 의미가 없으므로, 학습 단계에서 <span style="color: #2454ff;">**Masking**</span> 을 하게 된다. 
Masking 의 의미는, 이전 단어들에 대해서만 연관이 있고, 뒤에 있는 단어들에 대해서는 독립적으로 만드는 것을 말한다.

<br>

<img src="https://github.com/BlueYellowGreen/BlueYellowGreen.github.io/blob/main/.vuepress/public/assets/transformer/24.png?raw=true">

마지막 layer 에서 단어들의 분포를 만들어서, 매번 그 중에서 단어를 샘플링하는 방식으로 동작하게 된다.

<br>

여기까지 알아본 Transformer 는 사실 NMT 문제에서만 사용되었었는데, 점차 시간이 지나며 단어들의 sequence 를 다루는 것 뿐만이 아니라, 
<span style="color: #2454ff;">**이미지에서도 활용**</span>하고있다.

<span style="color: #2454ff;">**Vision Transformer**</span> &nbsp; &#10140; &nbsp; [**논문**](https://arxiv.org/abs/2010.11929)

이미지 분류를 할 때 인코더만 활용한다. 그리고 인코더에서 나오는 첫 번째 encoded vector 를 classifier 에 입력하는 구조이다. 
NMT 에서는 문장이 입력되었다면, 이미지에서는 일정 구간으로 나눠 이를 입력시켰다. 마찬가지로 positional encoding 이 들어간다.

<br>

그리고 문장이 주어지면 문장에 대한 이미지를 만들어내는 **DALL-E** 도 있다. openai 에서 말하길, transformer 에서 **디코더**만 활용했고, 
이미지를 16x16 ? grid 로 나눠 sequencial 하게 입력하고 문장도 입력해서, 새로운 문장이 주어졌을 때 그 문장에 대한 이미지를 만든다고 한다.

<span style="color: #2454ff;">**DALL-E**</span> &nbsp; &#10140; &nbsp; [**openai**](https://openai.com/blog/dall-e)

<br>

#### Generative Model

&nbsp; 기본적인 Generative Model 구조가 어떻게 되는지, 트렌드는 어떠한지 알아보자.
Generative Model 은 무엇일까? 생성 모델이라고 떠오르지만, 생성하는 것을 넘어서서 많은 것을 담고 있다. 강아지로 예시를 들어보면,
강아지처럼 생긴($x_{new}$) 샘플을 샘플링 (**generation**; $x_{new}\sim p(x)$) 하는 것을 할 수도 있고, 
<span style="color: #2454ff;">**Density estimation**</span> 을 할 수 있다. 이것은 <span style="color: #2454ff;">**anomaly detection**</span> (이상치 감지) 
에 적용될 수 있다. 그리고 데이터의 특징을 학습한다 (Unsupervised representation learning; <span style="color: #2454ff;">**feature learning**</span>).

&nbsp; 그래서, generative model 을 학습한다는 것은 단순히 <span style="color: #2454ff;">**생성**</span>해내는 것 뿐만 아니라 
<span style="color: #2454ff;">**구분**</span>도 할 수 있다는 것이다. 이러한 모델을 <span style="color: #2454ff;">**explicit**</span> model 이라고 부른다. 
<u>**입력이 주어졌을 때 확률 값을 얻어낼 수 있는 모델**</u>이다. 반대로 생성만 할 수 있는 모델은 inplicit model 이라고 한다.

그렇다면 $p(x)$ 를 어떻게 만들까? 이를 위해선 확률에 관한 지식이 필요하다.


**Basic Discrete Distributions**

- **Bernoulli distribution** &nbsp; &#10140; &nbsp; 0 or 1

    - $P(X=0)\;=\;p\quad\text{then}\quad P(X=1)\;=\;1-p$

    - $X\sim\text{Ber}(p)$ 라고 표현한다.

- **Categorical distribution** &nbsp; &#10140; &nbsp; N

    - $P(Y=i)\;=\;p_i\quad\text{such that}\quad\sum_{i=1}^{m}p_i\;=\;1$

    - $Y\sim\text{Cat}(p_1,\dots,p_m)$ 라고 표현한다.

<br>

&nbsp; RGB 로 예시를 들어보자.<br>
RGB 를 표현하면 $(r,g,b)\sim p(R,G,B)$ (joint distribution) 이고, $256 \times 256 \times 256$ 클래스를 갖는다. 
이것을 표현하려면 $256 \times 256 \times 256 - 1$ 개의 파라미터가 필요하다. 하나의 픽셀을 표현하기 위해 어마어마하게 필요하다..

&nbsp; RGB 는 너무 크니 binary (0, 1) pixel 를 표현해보자.
$n$ 개의 pixel 은 $2 \times 2 \times\dots\times2\;=\;2^n$ 경우가 존재한다. 이에 필요한 파라미터 수는 $2^n-1$ 이다. 이것도 많다. 
기계학습에서 파라미터가 많을 수록 학습하기 어려운데 말이다.

&nbsp; 여전히 $n$ 개의 pixel 을 표현하고 싶어서 파라미터를 줄여야 한다. 그러기 위해서 $n$ 개의 pixel 이 다 독립적이라고 가정하자.
사실 인접 픽셀은 연관되겠지만 말이다. 그래도 여전히 경우의 수는 $2^n$ 이지만, 이 distribution 을 표현하기 위해서는 
<span style="color: #2454ff;">**$n$**</span> 개의 파라미터만 있으면 된다!<br>
왜? 각각의 pixel 이 독립적이니깐, 더하면 된다!

&nbsp; 그런데 이 가정은 극단적이라 표현할 수 있는 정보가 별로 없다. 그래서 그 중간을 찾고 싶었다. 그래서 나온 것이 
<span style="color: #2454ff;">**Conditional Independence**</span> 개념이다. 그 중간을 위해 다음 3가지 규칙이 사용된다.

- **Chain rule**

    $$p(x_1,\dots,x_n)\;=\;p(x_1)p(x_2|x_1)p(x_3|x_1,x_2)\dots p(x_n|x_1,\dots,x_{n-1})$$

- **Bayes' rule**

    $$p(x|y)\;=\frac{p(x,y)}{p(y)}\;=\;\frac{p(y|x)p(x)}{p(y)}$$

- **Conditional independence**

    $$\text{if}\;x\;\bot\;y\;|\;z,\;\text{then}\;p(x|y,z)\;=\;p(x|z)$$

    - $z$ 가 주어졌을 때, $x$ 와 $y$ 가 independent 하다. (가정)

    - 그러면 $z$ 라는 random variable 이 주어졌을 때, $x$ 와 $y$ 가 independent 하니깐, 
    $x$ 라는 random variable 을 표현하는데 있어서 <span style="color: #2454ff;">**$y$ 는 상관이 없다**</span>.

<br>

이제 다시 파라미터를 따져보자.<br>
우선 Chain rule 만 이용하여 따져본다면 바뀔까? 다른 추가된 가정이 없는데 말이다.

- first pixel &nbsp; &#10140; &nbsp; $p(x_1)$ - 첫 pixel 은 1개의 파라미터로 표현할 수 있다.

- second pixel &nbsp; &#10140; &nbsp; $p(x_2|x_1)$ - 두 번째 pixel 은 2개의 파라미터가 필요하다.
    - $p(x_2|x_1=0)$ - $x_1$ 이 주어졌을 때 $x_2$ 이 0인 확률과, $p(x_2|x_1=1)$ - $x_1$ 이 주어졌을 때 $x_2$ 이 1인 확률

- third pixel &nbsp; &#10140; &nbsp; $p(x_3|x_1,x_2)$ - 세 번째 pixel 은 4개의 파라미터가 필요하다.

- 그러므로, $1+2+2^2+\dots+2^{n-1}\;=\;2^n-1$ 로 동일하다. 즉, 이전과 같다.

<br>

이제는 Conditional Independence 와 관련된 가정을 추가해보자 ( <span style="color: #2454ff;">**Markov assumption**</span> ).

$$\text{Markov assumption}\;:\quad \text{suppose}\;X_{i+1}\;\bot\;X_1,...,X_{i-1}|X_i\quad\text{then}\quad$$

$$p(x_1,...,x_n)\;=\;p(x_1)p(x_2|x_1)p(x_3|x_2)\dots p(x_n|x_{n-1})$$

<span style="color: #2454ff;">**i+1 번째 pixel 은 i 번째 pixel 에만 연관**</span>되어 있다는 가정이다. 
이것을 표현하기 위해 필요한 파라미터는 $\color{blue}{2n-1}$ 이다.

단순히 chain rule 로 쪼갠 것만 가지고는 파라미터 수의 변동이 없었지만, 쪼갠 뒤 Markov 가정을 적절히 적용하니 파라미터 개수를 줄일 수 있었다!
이러한 방법을 <span style="color: #2454ff;">**Auto-regressive model**</span> 이라고 부르고, **conditional independency** 하다.

<br>

&nbsp; 이제 Auto-regressive Model 을 살펴보자.<br>
$28\times28$ binary pixels 로 이루어진 MNIST 를 생각해보자. 우리가 하려는 것은 확률분포 $p(x)$ 를 구하는 것이다. 
여기에 활용하는 것은 Auto-regressive Model 로 Chain rule 을 가지고 Joint Distribution 을 나누어 사용해보려고 한다.
explicit, inplicit model 모두 살펴볼 것이다. 

그러면 이 $p(x)$ 를 어떻게 표현할 수 있을까? 말했던 것처럼 Chain rule 을 가지고 나누자.

$$p(x_{1:784})\;=\;p(x_1)p(x_2|x_1)p(x_3|x_{1:2})\dots$$

&nbsp; 한가지 짚고 넘어가야 하는 것은, Markov assumption 처럼 i+1 번째가 i 번째에만 dependent 한 것도 autoregressive 한 것이고, 
i+1 번째가 i 번째 뿐만 아니라 <span style="color: #2454ff;">**과거의 정보들과 dependent 한 것도 autoregressive 한 것**</span>이다. 
그리고 언급했던 가정에서 볼 수 있는 것 처럼 순서의 정보가 중요해서, 모든 random varables 에 ordering 을 해야 한다. 

&nbsp; 그런데 Image 에 순서를 어떻게 적용해야 할까? MNIST 예시의 경우에서도 784개의 픽셀에 순서를 적용해야 하는데, 
실제로 어떤 방식으로 순서를 부여하느냐에 따라 성능이 달라진다. 그리고 autoregressive 가 직전의 정보만 고려할 수도 있고, 이전의 n 개를 고려할 수도 있다 
(1개만 고려하는 것을 layer 1, n 개 고려하는 것을 layer n 이라고 부른다). 즉, 어떤식으로 conditional independcy 를 부여하느냐에 따라 모델의 structure 가 달라진다. 

&nbsp; 논문을 보자. 'NADE: Neural Autoregressive Density Estimator' 로 [**이곳**](https://arxiv.org/abs/1605.02226)에서 확인할 수 있다. 
여기서 사용한 방식이 0부터 직전까지의 데이터에 dependent 하게 설계하였다. 

$$p(x_i|x_{1|i-1})\;=\sigma(\alpha_ih_i+b_i))\quad\text{where}\quad h_i\;=\;\sigma(W_{<_ix_{1:i-1}}+c)$$

그래서 첫 번째 pixel 을 어느것에도 dependent 하지 않게 만들고, 두 번째 pixel 을 첫 번째 pixel 에만 dependent 하게 만든다. 
여기서 dependent 하다는 것은, 첫 번째 pixel 의 값을 받는 Neural Network 를 만들어서, single scala 가 나온 다음에 sigmoid 를 통과시키는 것이다. 
i 번째의 pixel 은 1번부터 i-1번째까지의 pixel 과 dependent 하다. Neural Network 입장에서 생각해보면, 입력이 점차 커져서 
<span style="color: #2454ff;">**weight 가 마찬가지로 커진다**</span>.

&nbsp; NADE 는 <span style="color: #2454ff;">**explicit model**</span> 이다. 단순히 generation 만 할 수 있는 것이 아니라, 
임의의 784개의 입력 즉, binary vector 가 주어지면, 이것에 대한 <span style="color: #2454ff;">**확률을 계산**</span>할 수 있다. 
어떻게 계산할까? 

$$p(x_1,...,x_{784})\;=\;p(x_1)p(x_2|x_1)\dots p(x_{784}|x_{1:784})$$

**Joint Distribution** 을 **Chain rule** 을 통해서 분리하고, 모델이 첫 번째 pixel 에 대한 확률분포를 알고 있고, 첫 번째 pixel 이 주어졌을 때 
두 번째 pixel 에 대한 확률분포를 알고 있고 ... 다 알고 있어서 각각을 independent 하게 계산할 수 있다. 
그래서 단순히 generation 만 할 수 있는 것이 아니라, 입력에 대한 확률을 계산까지 할 수 있다.

&nbsp; 지금 예시에서는 binary pixel 이기 때문에 output 을 sigmoid 를 통과시키는데, <span style="color: #2454ff;">**continous output**</span> 
이라면 마지막 layer 에 <span style="color: #2454ff;">**Gaussian mixture 모델**</span>을 활용하여 **continous 한 distribution** 을 만들 수 있다.

<br>

&nbsp; 다음은 <span style="color: #2454ff;">**Pixel RNN**</span> 이다.<br>
Image 속 pixel 을 만드는 모델 RNN auto-regressive 모델이다. $n\times n$ Image 가 있을 때,

$$p(x)\;=\;\prod_{i=1}^{n^2}\underset{\text{Prob. i-th R}}{p(x_{i,R}|x_{<i})}\underset{\text{Prob. i-th G}}{p(x_{i,G}|x_{<i},x_{i,R})}\underset{\text{Prob. i-th B}}{p(x_{i,B}|x_{<i},x_{i,R},x_{i,G})}$$

i 번째 pixel 에 R 을 만든 뒤, G 를 만들고 마지막에 B 를 만든다. 이전 모델과 차이점이 있다면, 이전 모델은 auto-regressive 모델을 FC Layer 를 통해 만들었다. 
i 번째 pixel 은 1부터 i-1 개의 입력을 다 고려할 수 있는 Neural Network ( **Dense Layer** ) 를 만든 것인데, 
Pixel RNN 은 이름처럼 **RNN** 로 만든 것이다. 그래서 RNN 을 통해서 generate 을 하는 것이다.

한 가지 외에에는, ordering 을 어떻게 하느냐에 따라서 <span style="color: #2454ff;">**Row LSTM**</span> 과 
<span style="color: #2454ff;">**Diagonal BiLSTM**</span> 으로 나눌 수 있다. **Row LSTM** 은 R 번째 pixel 을 만들 때 **윗쪽의 정보를 활용**하는 것이고, 
**Diagonal BiLSTM** 은 Bidirectional LSTM 을 활용하되, ordering 을 했을 때 **이전 정보들을 다 활용**한다.

<br>

**Variational Auto-encoder**

&nbsp; 이제 좀더 practical 한 모델을 살펴보자.<br>
<span style="color: #2454ff;">**Latent Variable Models**</span> 이며, D. Kingma, "Variational Inference and Deep Learning: A New Synthesis" 
Ph.D. Thesis 논문으로 Adam optimizer 도 만드셨다.

그 시작으로 <u><span style="color: #2454ff;">**Variational Auto-encoder**</span></u>을 알아보자. 우선 Variational inference (VI) 를 알아야 하는데, 
이것은 <span style="color: #2454ff;">**Posterior distribution: $p_\theta(z|x)$**</span> 를 찾는 것이 목적이다. 
Posterior distribution 이란, observation 이 주어졌을 때, 관심있어 하는 random variable 의 **확률분포**이다. 

그리고 <span style="color: #2454ff;">**Variational distribution: $q_{\phi}(z|x)$**</span> 의 목적은 계산하기 어려운 posterior distribution 을 
내가 학습할 수 있는, 최적화시킬 수 있는 것으로 <span style="color: #2454ff;">**근사하는 것**</span>이다. 
그리고 그 근사하는 분포가 Variational distribution 이다.

그래서 내가 원하는 Posterior distribution 을 잘 근사할 수 있는 Variational distribution 을 찾는 일련의 과정을 
<span style="color: #2454ff;">**Variational inderence**</span> 라고 한다. 그래서 무언가를 잘 찾겠다, 최적화 하겠다 하면 **Loss function** 이 필요하다. 
Variational inference 에서는 <span style="color: #2454ff;">**KL divergence**</span> metric 을 활용해서 variational distribution 과 
posterior distribution 과의 **차이를 줄인다.**

그렇다고 하는데, 뭔지도 모르는 posterior distribution 과 어떻게 차이를 줄일까? Variational Auto-encoder 논문에 나온 
<span style="color: #2454ff;">**ELBO**</span> 를 사용하는 것이다.

$$\text{In}\;p_\theta(D)\;=\;\mathbb{E}_{q_\phi(z|x)}[\ln p_\theta(x)]$$

$$=\;\mathbb{E}_{q_\theta(z|x)}\bigg[\ln\frac{p_\theta(x,z)}{p_\theta(z|x)}\bigg]$$

$$\quad\quad\;\;=\;\mathbb{E}_{q_\phi(z|x)}\bigg[\ln\frac{p_\theta(x,z)q_\phi(z|x)}{q_\phi(z|x)p_\theta(z|x)}\bigg]$$

$$\quad\quad\quad\quad\quad\quad\quad\quad\quad\;\;=\;\mathbb{E}_{q_\phi(z|x)}\bigg[\ln\frac{p_\theta(x,z)}{q_\phi(z|x)}\bigg]
+\;\mathbb{E}_{q_\phi(z|x)}\bigg[\ln\frac{q_\phi(z|x)}{p_\theta(z|x)}\bigg]$$

$$\quad\quad\quad\quad\quad\quad\quad\quad\quad\;=\underset{\text{ELBO} \uparrow}{\mathbb{E}_{q_\phi(z|x)}\bigg[\ln\frac{p_\theta(x,z)}{q_\theta(z|x)}\bigg]}
+\;\underset{\text{Objective} \uparrow}{D_{KL}(q_\phi(z|x)||p_\theta(z|x))}$$

궁극적으로 무엇을 이야기 하냐면, Variational distribution 과 Posterior distribution 과의 KL Divergence 를 줄이는 것이 목적(Objective $\downarrow$)인데 
이것이 불가능하니깐, <span style="color: #2454ff;">**ELBO (Evidence lower bound) 를 상대적으로 높이는 것**</span>이다. 그래서 뭔지도 모르는 
posterior distribution 에 근사할 수 있다. 

ELBO 는 더 나아가 Reconstruction Term 과 Prior Fitting Term 으로 나뉘어 질 수 있다 (decomposed).

$$\underset{\text{ELBO} \downarrow}{\mathbb{E}_{q_\phi}(z|x)\bigg[\ln\frac{p_\theta(x,z)}{q_\phi(z|x)}\bigg]}\;
=\;\int\ln\frac{p_\theta(x|z)p(z)}{q_\phi(z|x)}q_\phi(z|x)dz$$

$$\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
=\;\underset{\text{Reconstruction Term}}{\mathbb{E}_{q_\phi(z|z)}[p_\theta(x|z)]}\;-\;\underset{\text{Prior Fitting Term}}{D_{KL}(q_\theta(z|x)||p(z))}$$

여기서 Recontruction Term 을 통해 **auto-encoder** 의 <span style="color: #2454ff;">**reconstruction loss 를 최소화**</span>할 수 있고, 
Prior Fitting Term 은 latent distribution 을 <span style="color: #2454ff;">**prior distribution 과 유사**</span>하게 만든다. 
엄밀하게 따지면 explicit model 이 아닌 <span style="color: #2454ff;">**implicit model**</span> 이다.

그리고 **한계**도 존재한다.<br>
엄밀히 말하면 implicit model 이기 때문에, <span style="color: #2454ff;">**likelihood 를 계산하기 힘들다**</span>. 
그리고 ELBO 가 결국 reconstruction loss term 과 kl divergence term (prior fitting term)으로 나눠지는데, reconstruction 은 뭘 해도 되지만 
kl divergence 는 SGD 나 Adam 으로 최적화를 진행하기 때문에 <span style="color: #2454ff;">**미분가능**</span>해야 한다. 
그래서 <span style="color: #2454ff;">**diverse latent prior distribution 을 사용하기 힘들다**</span>. 
그래서 대부분의 **Variational Auto-encoder** 는 Gaussian prior 를 사용한다 
( 대부분의 경우 <span style="color: #2454ff;">**isotropic Gaussian 을 사용한다**</span> ).<br>
※ isotropic Gaussian 이란 모든 output dimension 이 independent 한 gaussian distribution 을 말한다.

$$D_{KL}(q_\phi(z|x)||\mathcal{N}(0,1))\;=\;\frac{1}{2}\sum_{i=1}^{D}(\sigma_{z_i}^2+\mu_{z_i}^2-\ln(\sigma_{z_i}^2)-1)$$

이러한 VA 를 통해 논문 속에서 얼굴을 생성한 결과를 확인할 수 있으며, 이를 통해 문장 등 여러가지를 생성할 수 있다.

<br>

**Adversarial Auto-encoder**

&nbsp; 앞에서 본 VA 의 가장 큰 단점은 **인코더**를 활용할 때, **prior fitting term** 이 **kl divergence** 를 사용한다는 것이라고 생각한다. 
그렇기 때문에 **gaussian 이 아닌 경우 활용하기 힘들다**. 그리고 많은 경우에 gaussian 을 사용하고 싶지 않을 수도 있다. 
그래서 AA (Adversarial Auto-encoder) 모델은 **GAN** 을 사용하여 latent distribution 사이의 분포를 맞춰준다. 
AA 는 VA 의 prior fitting term 속 **kl divergence** 를 <span style="color: #2454ff;">**GAN**</span> 으로 바꾼 것이라고 볼 수 있다.

그래서 <span style="color: #2454ff;">**latent distribution 을 sampling 가능한 분포만 있어도 이것과 맞출 수 있다**</span>. 
복잡하고 다양한 분포에 latent prior distribution 을 적용할 수 있다는 것이 AA 의 장점이다. Generation 성능도 VA 에 비해 좋은 경우가 많다. 

<br>

**Generative Adversarial Network**

&nbsp; 대망의 GAN 이다.

$$\underset{\text{G}}{\text{min}}\;\underset{\text{D}}{\text{max}}\;V(D,G)\;=\;\mathbb{E}_{x\sim p_{data}}(x)[\log D(x)]+\;\mathbb{E}_{z\sim p_z(z)}[\log(1-D(G(z)))]$$

GAN 의 큰 장점은, 결과로 나오는 generator 를 학습하는 <span style="color: #2454ff;">**discriminator 가 점차 좋아진다는 점**</span>인 것 같다. 
Discriminator 가 generator 를 통해서 점점 잘 되기 때문에, generator 도 같이 성능이 올라가서 좋은 이미지를 만들어낼 수 있는 것이다. 
GAN 또한 <span style="color: #2454ff;">**implicit model**</span> 이다. 

**Variational Auto-encoder** 의 경우 $p(z)$ distribution 속에서 $x$ &#10140; $p_\theta$ &#10140; $z$ &#10140; $q_\phi$ &#10140; $x$ 처럼, 
$x$ 가 인코더를 통해서 $z$ 로 갔다가 디코더를 통해서 원래 $x$ 도메인으로 가는 학습을 한다. 
그리고 generation 단계에서는 $p(z)$, latent distribution 에서 $z$ 를 샘플링해서 디코더를 통해 나오는 $x$ 가 generation 결과가 된다. 

하지만 <span style="color: #2454ff;">**GAN**</span> 은 좀 다르다.<br>
$\color{blue}z$ 라는 latent distribution 에서 출발해서 **G** 를 통해 **Fake** 가 나오고, 
**Discriminator** 는 이 **Fake** 와 **Real** 을 구분하는 분류기를 학습하고, 
**Generator** 는 그렇게 학습된 Discriminator 입장에서 True 가 나오도록 다시 generator 를 업데이트하고, 
Discriminator 는 이렇게 해서 나온 결과물들이 Real 과 discriminative 가 되도록 다시 학습한다 (D, G 를 번갈아가며 학습한다). 

**GAN Objective**

GAN Objective 는 Generator 와 Discriminator 사이의 minmax 게임으로 볼 수 있다. **Discriminator objective** 는 다음과 같다.

$$\underset{D}{\text{max}}\;V(G,D)\;=\;\mathbb{E}_{x\sim p_{data}}[\log D(x)]+\;\mathbb{E}_{x\sim p_G}[\log(1-D(x))]$$

그래서 위의 형태를 항상 **최적화** 시키는 discriminator 의 꼴은 다음과 같다.

$$D^\ast_G(x)\;=\;\frac{p_{data}(x)}{p_{data}(x)+p_G(x)}$$

Generator 가 고정되어 있을 때, 이것을 항상 최적으로 분리하는 Discriminator 는 $D^\ast_G$ 인 것이다. 
그래서 Generator 에 대해,

$$\underset{\text{G}}{\text{min}}\;V(G,D)\;=\;\mathbb{E}_{x\sim p_{data}}[\log D(x)]+\;\mathbb{x\sim p_G}[\log(1-D(x))]$$

**optimal discriminator** 를 **generator** 에 집어넣게 되면,

$$V(G,D^\ast_G(x))\;=\;\mathbb{E}_{x\sim p_{data}}\bigg[\log\frac{p_{data}(x)}{p_{data}+p_G(x)}\bigg]
+\;\mathbb{E}_{x\sim p_G}\bigg[\log\frac{p_G(x)}{p_{data}(x)+p_G{x}}\bigg]$$

$$\quad\quad\quad\quad\quad\quad\quad\;=\;\mathbb{E}_{x\sim p_{data}}\bigg[\log\frac{p_{data}(x)}{\frac{p_{data}(x)+p_G(x)}{2}}\bigg]
+\;\mathbb{E}_{x\sim p_G}\bigg[\log\frac{p_G{x}}{\frac{p_{data}(x)+P_G(x)}{2}}\bigg]-\log4$$

$$\quad\quad\quad\quad\;\;=\;\underset{2\times\text{Jenson-Shannon Divergence(JSD)}}{D_{KL}\bigg[p_{data},\frac{p_{data}+p_G}{2}\bigg]+\;D_{KL}\bigg[p_G,\frac{p_{data}+p_G}{2}\bigg]}-\log4$$

$$=\;2D_{JSD}[p_{data},p_G]-\log4$$

와 같이 된다. 이것의 의미는 <span style="color: #2454ff;">**true data distribution 과 학습한 generator 사이에 JSD 를 최소화 하는 것**</span>이다.

실제로 봤을 때, Discriminator 가 optimal discriminator 에 **수렴한다고 보장하기 힘들고**, 그렇게 됐을 때 generator 가 위의 수식처럼 나오지 않을 수도 있다. 
이론적으로는 말이 되지만, 현실적으로 JSD 줄이는 것은 의아한 부분이 있다. 

<br>

**DCGAN**

&nbsp; 처음의 GAN 은 multi layer perceptron (dense layer) 로 만들었고, Image 도메인으로 적용한 것이 DCGAN 이다. 
Deconvolutional Layer 를 가지고 generator 를 만듬으로써 성능을 높였다. 
그리고 Algorithm 적으로 좋아진 것은 없지만, **Leaky ReLU** 를 쓰는 등 여러 테크닉이 적용되었다. 

<br>

**Info-GAN**

&nbsp; 학습을 할 때, $z$ 를 통해서 단순히 Image 를 만들어내는 것이 아니라, 클래스라는 c 를 매번 랜덤하게 집어넣는다. 
random 한 one-hot vector 로 볼 수 있는데, 이를 통해 GAN 이 generate 할 때 특정 모드에 집중할 수 있게 한다. 

<br>

**Text2Image**

&nbsp; 문장이 주어지면 Image 를 생성한다. 입력이 문장이고, 이를 통해서 conditional gan 을 만들어서 Image 를 만들어내는 복잡한 네트워크를 가졌다.

<br>

**Puzzle-GAN**

&nbsp; Image 속에 subpatch 가 들어가 있으면, 이를 통해 Image 를 **복원**하는 네트워크를 GAN 을 가지고 만들었다.

<br>

**CycleGAN**

&nbsp; GAN 구조를 사용하면서, Image 사이의 도메인을 바꾼다. <span style="color: #2454ff;">**Cycle-consistency loss**</span> 를 활용한다. 
꼭 알아둬야 하는 컨셉인데, 예시로 말을 얼룩말로 바꾸려고 한다면, 똑같은 형태로 말이 있는 것과 얼룩말이 있는 이미지 2장이 필요한데, 
그럴 필요 없이 형태 구분 없이 말 사진들과 얼룩말 사진들만 있으면 된다. **GAN 구조가 2개** 들어가있는 형태이다. 

<br>

**Star-GAN**

&nbsp; 한국에서 나온 논문이고, Style Transfer 와 비슷한데 모드를 정해줄 수 있다는 점이 다르다. 즉, Image 를 조작할 수 있다는 점이 큰 특징이다. 

<br>

**Progressive-GAN**

&nbsp; 흥미로운 모델이다. **고차원의 이미지**를 잘 만들어낼 수 있는 방법론이다. $4\times4$ 라는 매우 coarse 한 Image 에서 출발해서, 
$1024\times1024$ 까지 점차 키워간다. Neural Network 에서 한 번에 고차원 Image 를 만들어내는 것이 힘드니깐, 점차 해상도를 높여가는 학습 ( **progressive training** ) 
방식을 사용했다. 

<br>

&nbsp; 이 외에도 무수히 많은 GAN 모델들이 있다. 그래서 모두 다 알기는 힘들지만, GAN 에 대한 중요한 아이디어들과, 
GAN 과 VA 가 어떻게 다르고, generate 를 하고 싶을 때 GAN 과 VA 모델 중 어떤 것을 쓸 것인지 판단할 수 있도록 학습하자.

<br>

::: details 참고
- [**Illustrated transformer (번역)**](https://nlpinkorean.github.io/illustrated-transformer/)
- [**PyTorch official Transformer tutorial**](https://pytorch.org/tutorials/beginner/transformer_tutorial.html)
- [**1시간 만에 GAN 완전 정복하기**](https://www.youtube.com/watch?v=odpjk7_tGY0&t=69s)
- [**An Introduction to Variational Autoencoders**](https://arxiv.org/abs/1906.02691)
:::

<br>

<hr>

### 피어 세션

<br>

<br>

<br>

<style scoped>
.tags { color: #2454ff; }
a { color: #2454ff; }
</style>