---
title: 01/26 Summary
lang: ko-KR

meta:
  - name: description
    content: 부스트캠프 AI Tech 01/26 Summary
  - property: og:title
    content: 01/26 Summary
  - property: og:description
    content: 부스트캠프 AI Tech 01/26 Summary
  - property: og:image
    content: 'https://boostcamp.connect.or.kr/images/pavicon_180_v2.ico'
  - property: og:url
    content: https://leedooho.com/AI/Boostcamp-AI-Tech/0126.html
---

### 2022/01/26, 새로 알게된 점

<p class="tags">#Save_&_Load #Transfer_learning #Monitoring</p>

&nbsp; 학습 결과를 남들에게 공유하거나(재현 및 전이 학습 용도로 사용), 자원이 부족한 경우 checkpoint 바탕으로 이어서 학습하기 위해 
모델을 **저장**하고 **불러올 줄 알아야 한다.** 특히나 나의 경우 GPU가 마땅치 않아 Colab 에서 주로 학습을 돌리는데, pro + 를 
구독하지 않는 이상, 런타임 초기화의 늪에서 벗어날 수 없다.

&nbsp; 그래서 Colab에 구글 드라이브를 마운트하고, 학습이 진행되며 **주기적으로 checkpoint 저장하는 방식**을 사용하면, 
압박감에서 조금이나마 벗어날 수 있다. 그리고 추가적으로 매 epoch 마다 metric 값을 slack, 더 나아가 카카오톡에 공유하도록 
해 놓으면 조금 더 그럴싸하다.

#### torch.save()

&nbsp; PyTorch에 구현되어 있는 `model.save()` 함수로 아주 쉽게 모델 아키텍처와 파라미터를 저장할 수 있다.

1. 모델 아키텍처와 파라미터 함께 저장 및 불러오기

```python
torch.save(모델_클래스_인스턴스, '저장할_경로/이름.pt')

model = torch.load('저장한_경로/이름.pt')
```

2. 모델 파라미터만 저장

```python
torch.save(모델_클래스_인스턴스.state_dict(), '저장할_경로/이름.pt')

model.load_state_dict(torch.load('저장된_경로/이름.pt'))
```

파라미터를 저장할 때 사용하는 `model.state_dict()` 은 각 레이어에서 **학습 가능한** 매개변수 및 **버퍼**를 dict 타입 객체로 불러온다. 
그리고 `state_dict()` 을 사용하려면, 해당 정보에는 모델 아키텍처 정보가 없으므로, 미리 동일한 모델 코드가 준비되어 있어야 한다.

참고로 여기서는 경로를 단순히 string 으로 작성했지만, OS 마다 경로 표현 방식의 차이가 있어서 하드 코딩보다는 `os.path.join()` 처럼 
OS에 맞게 경로를 생성해주는 함수를 사용하자.

여기서 더 나아가, **epoch, optimizer, loss** 등의 정보를 함께 저장해서 추후에 학습을 이어나갈 수 있도록 해보자.

```python
# save
torch.save({
    'model_state_dict': model.state_dict(),
    'epoch': epoch,
    'optimizer_state_dict': optimizer.state_dict(),
    'loss': loss,
    ...},
    f'checkpoint_epoch{epoch}_{loss}.pt')

# load
checkpoint = torch.load('경로/모델.pt')
model.load_state_dict(checkpoint['model_state_dict'])
epoch = checkpoint['epoch']
optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
loss = checkpoint['loss']
```

#### Transfer Learning

&nbsp; 요즘의 추세는 기존에 대용량 학습 데이터로 학습된 Backbone 모델을 이용하여, 
학습 데이터에 맞게 레이어를 변경해준 뒤 다시 학습을 시킨다. 
특히나 NLP의 경우, [**HuggingFace**](https://huggingface.co/models)에서 주로 전이학습시킬 Backbone 모델을 가져온다.

&nbsp; 전체 재학습을 시킬 수도 있지만, 기존 모델은 freeze 시키고, 새로 추가하거나 변형시킨 부분만 학습이 되도록 설정한다. 
그 방식은 아래의 코드를 이용하면 된다.

```python
for param in model.parameters():  # 모델에 있는 모든 파라미터에 대해서
    param.requires_grad = False  # 일단은 학습하지 않도록 freeze 시키고,

for param in model.linear_layers.parameters():  # 새롭게 변형시킨 레이어만
    param.requires_grad = True                  # 학습이 되도록 한다.
```

위의 예시에서 `.linear_layers` 부분은, class 개수에 맞게 마지막 레이어를 변경하는 경우가 많아 선택한 것이지, 
실제로는 추가하거나 **수정한 레이어에 대해서 freeze를 해제**하면 된다. 이외에도 특정 레이어에 접근해서 수정하는 방식은 
[**공식문서**](https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html#alexnet)를 참고하자.


#### Monitoring

&nbsp; 학습이 잘 진행되는지, 안된다면 그 원인을 파악하기 위해서 모니터링은 필수이다. 대표적인 모니터링 도구로는 
<span style="color: #2454ff;">**Tensorboard**</span> 와 <span style="color: #2454ff;">**WandB**</span> 가 있다.

<span style="color: #2454ff;">**Tensorboard**</span> 의 경우 TensorFlow 의 프로젝트로 만들어진 시각화 도구이지만 PyTorch 에서도 사용 가능하다. 
metric 등 상수 값을 표시하는 `scalar`, 모델의 computational graph 를 표시하는 `graph`, 
weight 같은 값의 분포를 표현하는 `histogram`, 실제 이미지와 예측 이미지를 표시하는 `image`, 
그리고 3d 형태의 데이터를 표현하는 `mesh` (어떤 식으로 사용되는지 감이 안온다.) 등 기능이 다양하다.

사용하기 위해서는 경로를 지정해야 한다. Tensorboard 는 크게 하나의 프로젝트로 보며, 하위 폴더를 개별 실험으로 취급한다. 

```python
import os
logs_dir = 'project_name'
os.makedirs(logs_dir, exist_ok=True)
```

프로젝트 로그 관리를 위한 폴더를 만들었으면, 실험 마다 하위 폴더를 설정해 기록한다.
<span style="color: #2454ff;">**6006 포트**</span>에서 확인 가능하다.

```python
from torch.utils.tensorboard import SummaryWriter

exp = f'{logs_dir}/exp_1'    # 개별 실험 공간을 위한 경로 지정
writer = SummaryWriter(exp)  # 해당 경로로 기록을 작성하기 위한 객체 생성

for epoch in range(epochs):  # 설명을 위한 가상의 학습 진행 과정
    for batch in datasets:
        # Loss/train 의 의미는,
        # Loss 라는 카테고리에 train 항목으로 기록한다는 뜻이다.
        writer.add_scalar('Loss/train', data, batch)
        writer.add_scalar('Loss/test', data, batch)
        writer.add_scalar('Accuracy/train', data, batch)
        writer.add_scalar('Accuracy/test', data, batch)
    writer.flush()
    
# 탭 레벨에 따라 배치마다 업데이트 되거나, 에폭마다, 아니면 학습 완료 후 업데이트 된다.

# 아래의 매직메서드로 Colab 에서 바로 확인 가능
# 보통 다른 셀에서 실행시킨다.
%load_ext tensorboard
%tensorborad --logdirs 'project_name'
```

<br>

<span style="color: #2454ff;">**WandB**</span> 의 경우 협업 시 큰 장점을 가지고, 이로 인해 MLOPs의 대표적인 툴로 부상하고 있다. 
개인 사용자의 경우 100GB 사용량에 한하여 사용이 자유롭고, 팀 단위도 학습이 목적일 경우 무료이다. 
[**Weights & Biases**](https://wandb.ai/wandb_fc/korean/reports/Weights-Biases-Data-Science---Vmlldzo4MDEwNzc)에서 설명하듯이, 
**Dashborad**, **Artifacts**(모델, 데이터 버전 관리), **Sweep**(Hyperparamter Optimization), **Report** 같은 
유용한 기능들을 제공한다.

사용법 또한 간단하다.<br>
wandb 라이브러리를 설치 후 congiguration 내용을 설정한 뒤, 매 학습마다 기록할 metric을 dict 타입으로 넘기면 된다.

```python
# !pip install wandb

config = {
    'epochs': epochs,
    'batch_size': batch,
    'learning_rate': lr
}

# wandb 상에 생성한 프로젝트 이름과 동일해야 한다.
wandb.init(project='project_name', config=config)

for epoch in range(epochs):
    ...
    wandb.log({'accuracy': acc, 'loss': loss})
```


<br>

<hr>

### 피어 세션

wandb 를 이용한 협업 연습 겸 경진대회에 참여해보자!

<br>

<br>

<br>

<style scoped>
.tags { color: #2454ff; }
a { color: #2454ff; }
</style>