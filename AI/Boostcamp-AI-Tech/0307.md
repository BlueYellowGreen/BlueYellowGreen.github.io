---
title: 03/07 Summary
lang: ko-KR

meta:
  - name: description
    content: 부스트캠프 AI Tech 03/07 Summary
  - property: og:title
    content: 03/07 Summary
  - property: og:description
    content: 부스트캠프 AI Tech 03/07 Summary
  - property: og:image
    content: 'https://boostcamp.connect.or.kr/images/pavicon_180_v2.ico'
  - property: og:url
    content: https://leedooho.com/AI/Boostcamp-AI-Tech/0307.html
---

### 2022/03/07, 새로 알게된 점

<p class="tags">#bag_of_words #Word2Vec #GloVe</p>

<br>

### NLP (Natural Language Processing)

NLP 의 주요 컨퍼런스 &#10140; **ACL**, **EMNLP**, **NAACL**

#### 자연어 처리 분야에서 다루는 다양한 Task

- **Low-level parsing**
    - Tokenization &nbsp; - &nbsp; 정보(단어 - token)단위로 쪼개는 task
    - Stemming &nbsp; - &nbsp; 다양한 어미의 변화 속에서 단어의 의미를 파악할 수 있도록 의미만 남기는 task (어근 추출)

<br>

- **Word and phrase level**
    - Named entity recognition (NER) &nbsp; - &nbsp; 단일 단어 혹은 여러 단어로 이루어진 고유 명사를 인식시키는 task
    - Part-of-speech (POS) tagging &nbsp; - &nbsp; 단어가 문장 내에서 어떤 성분(품사...)인지 알아내는 task
    - Noun-phrase chunking
    - Dependency parsing
    - Coreference resolution

<br>

- **Sentence level**
    - Sentiment analysis &nbsp; - &nbsp; 감정 분석 task
    - Machine translation &nbsp; - &nbsp; 기계 번역

<br>

- **Multi-sentence and paragraph level**
    - Entailment prediction &nbsp; - &nbsp; 두 문장간의 논리적인 내포, 혹은 모순 관계를 찾는 task
    - Question answering &nbsp; - &nbsp; 구글 검색에 적용되었다. 먼저 문서를 검색 후 해당 질문에 대한 답을 보여준다.
    - Dialog systems &nbsp; - &nbsp; 챗봇
    - Summarization &nbsp; - &nbsp; 한 줄 요약

<br>

<hr>

#### 자연어 처리에서 다루는 다른 분야

- **Text mining**
    - 주요 컨퍼런스 &#10140; KDD, The WebConf (formerly, WWW), WSDM, CIKM, ICWSM
    - 빅데이터 분석과 관련되는 경우가 많다.
    - 방대한 양의 문서 속에서 유의미한 정보 추출 (insights)
    - 문서 군집 (Document clustering; e.g., topic modeling)
    - 사회 과학과 밀접한 관계를 가지고 있다.

<br>

- **Information retrieval**
    - 주요 컨퍼런스 &#10140; SIGIR, WSDM, CIKM, RecSys
    - 구글이나 네이버에서 사용하는 검색 기술을 연구하는 분야
    - 어느정도 해당 분야가 성숙한 경지에 이르러서, 발전 속도가 느리다.

<br>

<hr>

#### Bag-of-Words

&nbsp; 딥러닝이 적용되기 이전에 사용되는 방법이다. 문서를 숫자로 나타내는 방식이다.

1. 주어진 모든 문장에 대해서 **unique words** 를 모아서 단어장(사전 - 중복 제거)을 만든다.

2. 각각의 단어를 one-hot vector 로 인코딩한다.

    - 각각의 단어 사이의 거리는 $\sqrt{2}$ 가 된다.

    - 각각의 단어간의 cosine similarity 는 0 이 된다.

3. 문장을 one-hot vector 로 표현한다. (각 차원(단어)에 중복되는 개수를 작성)

<br>

이제 bag-of-words vector 로 나타낸 문서를, 정해진 카테고리/클래스로 분류할 수 있는 **NaiveBayes Classifier** 에 대해 알아보자.

우선 문서가 분류될 수 있는 카테고리/클래스가 c 개 있고 문서가 d 개 있다고 가정한다. 
그렇다면 문서가 c 개의 각각에 클래스에 속할 확률 분포는 다음과 같다.

$$c_{\text{MAP}}\;=\;\underset{c\in C}{\text{argmax}}\;P(c|d)$$

이러한 방식은 **maximum a posteriori** 로 부르게 된다. 이는 bayes rule 에 의해서 다음과 같은 식으로 나타낼 수 있다.

$$\underset{c\in C}{\text{argmax}}\;\frac{P(d|c)P(c)}{p(d)}$$

여기서 $P(d)$ 는 특정한 문서 $d$ 가 뽑힐 확률을 의미한다. 그런데 여기서 우리는 문서 $d$ 가 고정되었다고 볼 수 있기 때문에, 
상수 값으로 생각할 수 있다. 그러면 argmax 계산 상에서 제외할 수 있다.

$$\underset{c\in C}{\text{argmax}}\;P(d|c)P(c)$$

위 식에서 $P(d|c)$ 는 특정 카테고리 $c$ 가 고정되었을 때, 문서 $d$ 가 나타날 확률을 의미한다. 
그리고 문서 $d$ 는 첫 번째 단어 $w_1$ 부터 마지막 단어 $w_n$ 까지 동시에 나타나는 **동시 사건**으로 볼 수 있다. 
각 단어가 등장할 확률이, $c$ 가 고정되어 있는 경우 독립이라고 가정할 수 있다. 
그러면 각 단어가 나타날 확률을 곱한 식으로 나타낼 수 있다.

$$P(w_1,\dots,w_n|c)P(c)\quad\rightarrow\quad P(c)\prod_{w_i\in W}P(w_i|c)$$

모르는 단어가 나왔을 경우, 해당 단어에 대한 확률은 0으로 추정된다. 
그러면 해당 단어가 포함된 문장의 확률은 0으로 계산된다는 문제가 있다. 
이러한 문제를 해결하고자 다양한 regularization 기법이 naivebayes classifier 에 적용된다.

<br>

<hr>

#### Word2Vec

&nbsp; Word Embedding 이라는 기법은 자연어가 단어를 정보들의 단위로 보아 단어들의 sequence 라고 볼 때, 각 단어들을 특정한 차원으로 이루어진 공간상의 한 점, 혹은 좌표로 변환하는 **벡터로 변환하는 기법**이다. 
Word Embedding 자체가 머신러닝이 적용된 알고리즘으로써, 학습데이터를 입력하면 **유사한 정보는 공간상에 가까이** 두도록, 유의미한 좌표를 생성한다. 
그래서 두 단어 벡터 사이의 거리가 가까울 수록 유사한 의미를 가진다고 볼 수 있다. 


Word Embedding 을 학습하는 방법 중 유명한 방법인 **Word2Vec** 에 대해 알아보자.<br>
비슷한 의미를 가진 단어가 공간상에서 가까운 좌표로 가지도록 학습하기 위해서, Word2Vec 알고리즘은 **같은 문장에서 나타난 인접한 단어들간에 그 의미가 비슷할 것이라고 가정**한다. 
그래서 특정 단어 주위에 나타나는 단어들에 대한 확률 분포를 예측한다. $P(w|\text{word})$ <br>
특정 단어를 입력으로 주고 **주변 단어들을 숨긴채** 이를 예측하도록 Word2Vec 모델을 학습한다.

- Analogy Reasoning - 단어들간의 관계
- Intrusion Detection - 단어들 중 가장 의미가 상이한 단어 찾기

이 외에도 Word2Vec 은 다양한 자연어 처리 task 에서 자연어를 word 단위의 vector 로 나타내어 그 task 의 입력으로 사용하기 위해 사용되기도 한다.

- Word similarity
- Machine translation
- Part-of-speech (PoS) tagging
- Named entity recognition (NER)
- Sentiment analysis
- Clustering
- Semantic lexicon building
- Image captioning


<br>

<hr>

#### GloVe

&nbsp; Word2Vec 과 더불어 많이 쓰이는 GloVe (Global Vectors for Word Representation) 에 대해 알아보자.<br>
Word2Vec 과의 가장 큰 차이점은, 두 단어 쌍이 한 윈도우 내에서 총 몇번 동시에 등장하는지 미리 계산하고,  
그 정보를 토대로 내적 값이 가까워질 수 있도록 하는 새로운 Loss Function 을 사용했는 것이다.

$$J(\theta)\;=\;\frac{1}{2}\sum_{i,j=1}^{W}f(P_{ij})(u_i^Tv_j-\log{P_{ij}})^2$$

Word2Vec 에서는 특정한 입출력 쌍이 빈번하게 등장했을 경우, 해당 item 이 여러번에 걸쳐 학습됨으로써 비례적으로 내적값이 커지도록 하는 반면, 
GloVe 에서는 동시에 등장한 횟수를 미리 계산하고, 이를 사용함으로써 중복된 계산을 피할 수 있다. 
따라서 상대적으로 학습이 빠르게 진행된다. 
그리고 GloVe 는 추천시스템의 관점에서도 볼 수 있다.


<br>

<br>

<style scoped>
.tags { color: #2454ff; }
.img-to-center { display:block; margin: 0 auto; }
a { color: #2454ff; }
</style>