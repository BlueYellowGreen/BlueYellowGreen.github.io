---
title: 03/08 Summary
lang: ko-KR

meta:
  - name: description
    content: 부스트캠프 AI Tech 03/08 Summary
  - property: og:title
    content: 03/08 Summary
  - property: og:description
    content: 부스트캠프 AI Tech 03/08 Summary
  - property: og:image
    content: 'https://boostcamp.connect.or.kr/images/pavicon_180_v2.ico'
  - property: og:url
    content: https://leedooho.com/AI/Boostcamp-AI-Tech/0308.html
---

### 2022/03/08, 새로 알게된 점

<p class="tags">#rnn #lstm #gru</p>

<br>

### RNN

RNN 의 구성요소는 다음과 같다.

- $h_{t-1}$ &nbsp; - &nbsp; 이전 hidden-state vector

- $x_t$ &nbsp; - &nbsp; 특정 time step 에서의 input vector

- $h_t$ &nbsp; - &nbsp; 새로운 hidden-state vector

- $f_W$ &nbsp; - &nbsp; 파라미터가 $W$ 인 RNN 함수

- $y_t$ &nbsp; - &nbsp; time step t 에서의 output vector

$$h_t\;=\;f_w(h_{t-1},x_t)$$

$$\downarrow$$

$$h_t\;=\;\tanh(W_{hh}h_{t-1}+W_{xh}x_t)$$

$$y_t\;=\;W_{hy}h_t$$

<br>

#### RNN Types

- one-to-one &nbsp; - &nbsp; sequence 정보가 없다.

- one-to-many &nbsp; - &nbsp; Image Captioning

    - 첫 번째에만 유의미한 입력이 들어가고, 나머지 부분의 입력은 0으로 채워진 tensor 가 입력으로 들어간다.

- many-to-one &nbsp; - &nbsp; sequence text 가 입력으로 들어간다. (감정 분석)

- many-to-many &nbsp; - &nbsp; Machine Translation (입력을 다 받은 후 출력하기 시작)

- many-to-many &nbsp; - &nbsp; Video classification on frame level (입력마다 출력, 단어마다 품사 추정)

<br>

#### Character-level Language Model

Language model 이라고 불리는 task 는, 문자열이나 단어들의 순서를 바탕으로 다음 단어를 맞추는 task 이다. 
Vocab 을 구성 후, embedding vector 를 입력으로 사용한다.

ex) hello 에서 h 를 입력하면 바로 e 를 예측해야 하고, e 를 입력하면 바로 l 을 예측해야 한다. (many-to-many)

$$h_t\;=\;\tanh(W_{hh}h_{t-1}+W_{xh}x_t+b)$$

각각의 time step output 을 구하기 위해 $W_{hy}h_t+b$ (**Logit**)를 계산한다. &#10140; **Softmax**

학습 과정중 back propagation 을 통해 $W_{hy}$, $W_{hh}$, $W_{xh}$ 를 업데이트 한다.

<br>

모델을 학습 시킨 후, inference 과정에서는 **첫 번째 입력만 주고** 해당 예측값을 다음 state 의 입력으로 사용한다. 

그리고 character-level language model 설명 중 예시로 hello 를 사용했지만, 단어가 아니라 문단을 학습할 수도 있다. 
**공백도 Vocab 에 포함**시켜야 한다. 학습 초반에는 한 글자만 입력 시 의미없는 글자의 나열로 추론되지만, 학습이 진행될 수록 
그럴싸한 문장이 생성된다. 

또한 다른 예시로써, RNN 을 이용하여 논문을 작성할 수도 있다. Latex 을 학습 데이터로 사용한다. 
그리고 C 언어 코드를 학습 데이터로 사용하여 프로그램을 만들 수도 있다. (들여쓰기를 학습시키기 위해 공백이 Vocab 에 포함되어야 한다)

<br>

이러한 RNN 모델은 sequence 를 한 번에 입력하여 학습시켜야 하기 때문에, sequence 길이에 대한 제약이 있다. 
그래서 truncation 이라고, seguence 의 길이를 제한하여 진행한다. 
`range(0, len(sequence), truncation_length)` 마다 학습을 시키는 느낌이다.

그래도 여전히 sequence 가 길다면 vanishing gradient 문제가 발생한다. 

<br>

<hr>

### LSTM

**Long Short-Term Memory**

RNN 의 문제점이었던 Gradient Vanishing / Explosion 을 해결하고, time step 이 먼 경우에도 필요한 정보를 보다 
효과적으로 처리하고 학습할 수도 있도록 하는 LSTM 구조가 고안되었다.

$$h_t\;=\;f_W(x_t,h_{t-1})$$

$$\{c_t, h_t\}\;=\;\text{LSTM}(x_t,c_{t-1},h_{t-1})$$

<br>

LSTM 의 대표적인 구성요소는 다음과 같다.

- i &nbsp; - &nbsp; Input gate. cell 에 기록할지 말지를 판단한다. (sigmoid)

- f &nbsp; - &nbsp; Forget gate. cell 에서 지울지 말지를 판단한다. (sigmoid)

- o &nbsp; - &nbsp; Output gate. cell 을 얼마만큼 드러낼지 판단한다. (sigmoid)

- g &nbsp; - &nbsp; Gate gate. cell 에 얼마만큼 기록할지 판단한다. (tanh)

<br>

<hr>

### GRU

**Gated Recurrent Unit**

GRU 는 LSTM 의 구조를 보다 경량화해서 적은 메모리 및 빠른 시간으로 학습시킬 수 있다.<br>
LSTM 의 cell state vector 와 hidden state vector 를 일원화하여, **오직 hidden state vector 만이 존재**한다. 
전체 동작 원리는 LSTM 과 유사하다.

- $z_t\;=\;\sigma(W_z\cdot[h_{t-1},x_t])$

- $r_t\;=\;\sigma(W_r\cdot[h_{t-1},x_t])$

- $\tilde{h_t}\;=\;\tanh(W\cdot[r_t\cdot h_{t-1},x_t])$

- $h_t\;=\;(1-z_t)\cdot h_{t-1}+z_t\cdot\tilde{h_t}$

- $\text{c.f)}\;C_t\;=\;f_t\cdot C_{t-1}+i_t\cdot\tilde{C_t}\quad\text{in LSTM}$

<br>

LSTM 에 비하여 성능이 뒤쳐지지 않아, LSTM 과 더불어 많이 사용됐다. 
그리고 곱셈 연산이 이루어졌던 RNN back propagation 에 비해 **덧셈 연산**으로 이루어진 LSTM 과 GRU 의 back propagation 으로 인해 
gradient vanishing 문제가 다소 해결되었다.

<br>

<br>

<style scoped>
.tags { color: #2454ff; }
.img-to-center { display:block; margin: 0 auto; }
a { color: #2454ff; }
</style>