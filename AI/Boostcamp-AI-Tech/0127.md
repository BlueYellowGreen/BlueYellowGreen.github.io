---
title: 01/27 Summary
lang: ko-KR

meta:
  - name: description
    content: 부스트캠프 AI Tech 01/27 Summary
  - property: og:title
    content: 01/27 Summary
  - property: og:description
    content: 부스트캠프 AI Tech 01/27 Summary
  - property: og:image
    content: 'https://boostcamp.connect.or.kr/images/pavicon_180_v2.ico'
  - property: og:url
    content: https://leedooho.com/AI/Boostcamp-AI-Tech/0127.html
---

### 2022/01/27, 새로 알게된 점

<p class="tags">#Multi-GPU #Hyperparameter_Tuning #Troubleshooting</p>

#### Multi-GPU

&nbsp; 하드웨어 성능의 향상으로 딥러닝 붐이 찾아왔고, 지금도 새로운 기술(논문)들이 나오고 있지만 
사용되는 모델은 점점 정해져 가는 것 같다. 그래서 모델 구조 변경을 통해 성능을 향상시키는 것 보다, 
**더욱 더 많은 데이터**를 학습시켜 모델의 성능을 향상시키는 추세이다.

&nbsp; 2018년 BERT에 비해 2019년 GPT-2 는 10배 가량, 또 2020년 GPT-3는 GPT-2 대비 10배 가량 모델의 사이즈가 커졌다. 
그래서 이들 모델을 학습시키는 시간도 늘어났다. BERT-Large 의 경우 Nvidia Tesla V100 32G * 8 GPU 서버 한 대로 
Pre-training 에만 약 2주의 시간이 걸린다. 그러다보니, 분산처리 학습은 선택이 아닌 필수로 다가오고 있다.

&nbsp; 분산처리 학습에 대해 알아보기에 앞서서 용어 정리부터 하자. 두 개의 단어만 구분하면 된다. 
<span style="color: #2454ff;">**GPU**</span> vs. <span style="color: #2454ff;">**Node**</span> . 
GPU 는 말 그대로 그래픽카드이고, Node 는 System 을 의미한다. 즉 하나의 서버이다. 그래서 일반적인 개인 학습자 환경에서 
'GPU' 여러개가 있다고 하면, 하나의 컴퓨터에 Multi-GPU 환경을 구성한 **Single Node Multi-GPU** 인 것이다.

&nbsp; 딥러닝에서 Multi-GPU를 이용한 학습은 예전부터 사용했다. AlexNet 논문에서 해당 내용을 살펴볼 수 있는데, 
그 전에 병렬 학습이 두 가지 방식으로 나뉜다는 것을 알아야 한다. 첫 번째는 <span style="color: #2454ff;">**Model Parallel**</span>이고, 
두 번째는 <span style="color: #2454ff;">**Data Parallel**</span>이다.

&nbsp; **Model Parallel**은 이름에서 볼 수 있듯이, 모델을 나누는 것이다. AlexNet 에서 사용한 방식인데, 매우 까다롭다. 
병렬처리 학습의 핵심은 병목 현상을 최소화 시켜 학습 시간을 단축시키는 것인데, 모델 간의 배치 학습 정보를 공유하는 파이프라인이 
겹치지 않는다면 일반 학습과 다를바가 없다. 그래서 최대한 겹치도록 파이프라인을 짜는 것이 중요한데, 그것이 어렵다..

&nbsp; 그 다음은 **Data Parallel**이다. 데이터를 나눠 GPU 에 할당한 뒤, 결과의 평균을 취하는 방식으로 가장 많이 사용되는 병렬처리 학습 방식이다. 
PyTorch 에서는 Multi-GPU 학습을 쉽게 하도록 잘 구현되어 있다. `DataParallel(model)` 과 `DistributedDataParallel(model, device_ids=[])` 이다. 
전자의 방식이 간편하여 주로 사용하게 될텐데, DataParallel 은 단순히 데이터를 분배한 후 평균을 구한다. 사용 방식은 간단하지만, 
GPU 사용 불균형 문제가 발생할 수도 있다. 반면, DistributedDataParallel 은 각 GPU 를 위한 CPU 프로세스를 생성하여 GPU에 할당시킨다. 
그러다보니 멀티프로세싱 통신 규약을 정의 과정이 필요하다. 결국 모두의 방식을 알아두면 좋지만, 현재 Multi-GPU 를 사용할 수 있는 환경이 없어서 
테스트를 해보지 못하고 있다...

#### Hyperparameter Tuning

&nbsp; 다음 주제는 하이퍼파라미터 튜닝이다. 배웠던 내용 중에 가슴에 남은 내용은, 하이퍼파라미터 튜닝보단 데이터를 어떻게 더 모을지, 전처리할 지에 
에너지를 쏟으라는 것이다. 그게 성능 향상 폭이 크다. 그래도 이러한 과정이 다 끝나고 마지막에 조금의 성능 향상을 위해 튜닝 개념은 반드시 알아야 한다. 

&nbsp; 예전에 주로 사용한 하이퍼파라미터 튜닝 방식은, `grid`로 큰 범위를 찾고, 그 다음에 해당 주의로 `random` 방식을 사용하여 찾는 것이다. 
최근에는 <span style="color: #2454ff;">**BOHB**</span> 라고, 베이지안 기법을 이용한 최적화 방식을 사용하기도 한다. 
이외에도 최적의 모델 구조를 찾는 <span style="color: #2454ff;">**NAS**</span> (Neural Architecture Search), <span style="color: #2454ff;">**AutoML**</span> 
등이 있는데, 우리는 파이썬 라이브러리인 <span style="color: #2454ff;">**Ray**</span> 에 대해 알아본다.

&nbsp; **Ray** 는 Multi Node Multi Processin 을 지원하는 모듈로써, ML/DL 을 병렬처리하기 위해 개발되었다. 
현재 분산 병렬 학습을을 위한 표준으로 자리잡았고, 이중 <span style="color: #2454ff;">**tune**</span> 을 이용하여 하이퍼파라미터 튜닝을 할 수 있다. 

&nbsp; 방식은 크게,

1. configuration 에 **탐색 공간**을 지정하고,

2. **학습 스케줄링 알고리즘**을 무엇을 사용할지 지정하고,

3. 결과 **출력 양식**을 지정한 뒤,

4. **병렬 처리 양식**으로 학습을 진행하면 된다.

```python
config = {
    'l1': tune.sample_from([l1_탐색범위지정]),
    'l2': tune.sample_from([l2_탐색범위지정]),
    'lr': tune.loguniform([leaning_rate_탐색범위시작, leaning_rate_탐색범위끝),
    'batch_size': tune.choice([2, 4, 8, 16, ...])
}
```

위의 코드처럼 탐색하고자 하는 metric 등의 범위를 configuration 으로 선언한다.

```python
scheduler = ASHAScheduler(
    metric='loss',
    mode='min',
    max_t=max_epoch,
    grace_period=1,
    reduction_factor=2)
```

그리고 어떤 식으로 학습을 진행시킬 지 알고리즘을 선택한다. ASHA 방식은 탐색 과정 중 의미가 없다고 판단되는 metric 들을 중도에 포기하여, 
탐색 속도를 높인다.

```python
reporter = CLIReporter(
    metric_columns=['loss', 'accuracy', 'training_iteration'])
```

탐색 과정을 CLI 환경에서 확인하도록 CLIReporter 를 사용하고, 어떤 정보를 확인할 것인지 입력한다.

```python
result = tune.run(
    partial(train_cifar, data_dir='경로'),
    resources_per_trial={'cpu': 2, 'gpu': gpus_per_trial},
    config=config,
    num_samples=num_samples,
    scheduler=scheduler,
    progress_reporter=reporter)
```

마지막으로 병렬 처리 방식으로 학습을 진행하면 된다. 이 또한 Multi-GPU 환경을 접하지 못해 직접 해보진 못했지만, 알아둬야 한다고 생각한다.

#### Troubleshooting

OOM (Out Of Memory) 문제를 해결하려면?

- `nvidia-smi` 처럼 파이썬 라이브러리인 `GPUtil` 을 이용하여 매 epoch 마다 메모리가 늘어나는지 확인해보자.

- `torch.cuda.empty_cache()` 를 써보자. 학습시 이전 학습 내용 캐시가 남아있을 수도 있어서, 학습 직전에 삭제시키자. `del`은 사용하더라도 
GC 가 수거하기 전까지는 메모리를 잡아먹는다.

- loop 계산 속 tensor 연산을 피하자. tensor 로 처리된 변수는 연산 시 GPU 메모리를 사용하는데, 메모리 buffer 까지 생겨 더욱 메모리를 잡아먹는다. 
그래서 연산이 필요하다면 ( ex. epoch 속 매 batch 별 loss 합 ), python 기본 객체로 변환해서 처리해야 한다. 
`.item()` 을 붙이거나 `float()` 등을 사용해 기본 객체로 만들자.

- batch 사이즈를 1로 학습을 진행하여 코드 상의 오류는 아닌지 확인하자.

- validation 및 inference 에서는 학습이 필요 없으므로 `torch.no_grad()` 를 통해 메모리에서 자유로워지자.

이 외에도,

- 꼭 필요한 경우에만 Linear, LSTM 을 사용하자.

- 파이썬 라이브러리인 <span style="color: #2454ff;">**torchsummary**</span>을 사용하여 사이즈 입력 형태를 맞추자.

<br>

<hr>

### 피어 세션

NPU 같이 AI 전용 칩을 위한 AI 전용 컴파일러를 ETRI 에서 개발하였다.

일단 소식은 알아두자..

<br>

<br>

<br>

<style scoped>
.tags { color: #2454ff; }
a { color: #2454ff; }
</style>