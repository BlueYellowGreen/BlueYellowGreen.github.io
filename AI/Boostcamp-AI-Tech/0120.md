---
title: 01/20 Summary
lang: ko-KR

meta:
  - name: description
    content: 부스트캠프 AI Tech 01/20 Summary
  - property: og:title
    content: 01/20 Summary
  - property: og:description
    content: 부스트캠프 AI Tech 01/20 Summary
  - property: og:image
    content: 'https://boostcamp.connect.or.kr/images/pavicon_180_v2.ico'
  - property: og:url
    content: https://leedooho.com/AI/Boostcamp-AI-Tech/0120.html
---

### 2022/01/20, 새로 알게된 점

<p class="tags">#vector #matrix #gradien_descent #SGD #Front_Propagation #Back_Propagation</p>

&nbsp; 본격적으로 AI를 위한 Math 학습이 진행되었다.<br>
고대하던 수업을 받았는데 흥미로웠다. 다만 수식적인 부분에서는 완벽하게 이해한 것이 아니라서, 
추후에 수식을 직접 풀어나가며 이해하려고 한다.

#### Vector

&nbsp; `[1, 2]` 처럼 표시되는 이 vector는 공간에서 한 점이라고 해석하기도 하고, 
원점으로부터 <span style="color: #2454ff;">**상대적인 위치**</span>라고 해석하기도 한다.
여기서 원점으로부터 상대적인 위치로 해석하게 되면 <span style="color: #2454ff;">**노름(norm)**</span>이라는 개념이 나온다.

&nbsp; Norm은 <span style="color: #2454ff;">**임의의 차원**</span>에 대해서 원점에서부터의 거리를 의미한다.
그 중에서도 $L_1$ - norm과 $L_2$ - norm 에 대해서 알아야 한다. $L_1$ 은 변화량의 절대값을 모두 더한 값이다. 
이에 반해, $L_2$ 는 유클리드 거리로서 제곱 합의 평균이다.

$$||x||_1=\sum^{d}_{i=1}|x_i|\;\;\;\;\;\;\;\;||x||_2=\sqrt{\sum^{d}_{i=1}{|x_i|}^2}$$

&nbsp; 그러다보니, $L_1$ 은 절댓값의 합이라 $L_2$ 에 비해 이상치에 대해 덜 민감하여 <span style="color: #2454ff;">**robust**</span>한 상황이나 
<span style="color: #2454ff;">**Lasso Regression**</span>에 주로 적용된다. 반대로 $L_2$ 는 $L_1$ 에 비해 이상치에 민감하여, 
<span style="color: #2454ff;">**outlier 탐지**</span>하는 곳이나, <span style="color: #2454ff;">**라플라스 근사**</span>, 
그리고 <span style="color: #2454ff;">**Lasso Regression**</span>에 주로 적용된다.

#### Matrix

&nbsp; Matrix 에 대한 접근법을 <span style="color: #2454ff;">**vector 공간에서 사용되는 연산자**</span>라고 생각하자.<br>
그렇다면 **matrix product**(행렬곱)은 vector를 <span style="color: #2454ff;">**다른 차원의 공간**</span>으로 보낸다고 볼 수 있다. 
반대로, **inverse matrix**(역행렬)은 다른차원에 있던 vector를 되돌리는 것인 셈이다.

&nbsp; 문제는 역행렬이다. 기존에 배우기론, 역행렬은 row와 column의 개수가 같고 determinant가 0이 아닌 경우에만 구할 수 있다고 했다. 
하지만 Tabular 데이터에 대해 **선형 회귀**를 진행(근사 함수의 **계수를 찾기**)하려고 하면 일반적으로는 row의 개수가 column의 개수보다 월등히 많다.

&nbsp; 이렇게 row와 column이 다른 경우에 대해서 <span style="color: #2454ff;">**유사역행렬(pseudo-inverse)**</span> 
또는 <span style="color: #2454ff;">**무어-펜로즈(Moore-Penrose)**</span>를 이용하여 <span style="color: #2454ff;">**역행렬을 근사**</span>할 수 있다. 
이러한 역행렬을 보통 $A_{(n\times m)}$에 대해 $A^+$ 같이 `+`를 붙여 표현하며, 다음의 식으로 계산할 수 있다.

$$n \; \geq \; m \;\;\;\; \rightarrow \;\;\;\; A^+ \; = \; (A^TA)^{-1}A^T$$
$$n \; \leq \; m \;\;\;\; \rightarrow \;\;\;\; A^+ \; = \; A^T(AA^T)^{-1}$$

하지만, 우리는 앞으로 경사하강법을 사용한다.

#### 경사하강법

&nbsp; 그라디언트 벡터 $\nabla$ 는 <span style="color: #2454ff;">**가장 빨리 증가하는 방향**</span>으로 흐른다. 
그래서 각 변수에 대해 $-\nabla$ 를 적용하면 **최솟값**을 구할 수 있다. 다만 여기서 **무엇에 대한 최솟값**을 구해야 하는지가 핵심이다. 
우리는 학습을 통해 어떠한 데이터를 넣으면 **최대한 정확한 예측값**을 얻고 싶다. 여기서 예측을 하기 위해서는, 답에 가까울 것이라고 예상하는 함수나 
분포에 대한 정보가 있어야 데이터를 입력해 계산할 수 있다. 이러한 것을 찾는 과정은, 주어진 데이터와 레이블(답)을 이용한다.

$$||y-X\beta||_2\quad\quad\leftarrow\quad\quad\text{최소화}$$

$$\rightarrow\quad\quad\nabla_\beta\underset{||y-X\beta||^2_2\text{사용가능}}{||y-X\beta||_2}\;
=\;(\partial_{\beta_1}||y-X\beta||_2,\;...,\;\partial_{\beta_d}||y-X\beta||_2)$$

$$\rightarrow\quad\quad\partial_{\beta_k}||y-X\beta||_2\;
=\;\partial_{\beta_k} \Bigg\{
\underset{\rightarrow\;\text{RMSE}\;=\;\sqrt{\text{MSE}}\;=\;\frac{\text{SSE}}{\text{degree}}}
{\frac{1}{n}\sum^{n}_{i=1}\Bigg(y_i-\sum^{d}_{j=1}X_{ij}\beta_j}\Bigg)^2\Bigg\}^{\frac{1}{2}}$$

&nbsp; **&nbsp; SSE** 는 오차의 제곱 합으로, 인과관계를 제대로 설명하는지 측정하기 위해 **잔차의 합**을 사용한다. 
테스트 데이터를 이용한 계산에서는 자유도로 나누는 것이 아닌 $n$ 으로 나눠야 한다. 알려지지 않은 복잡성은 MSE 계산에 들어갈 수 없기 때문이다.

$$\frac{1}{n}\sum^{n}_{i=1}\Bigg(y_i-\sum^{d}_{j=1}X_{ij}\beta_j\Bigg)^2\quad\text{assume as}\quad A
\quad\rightarrow\quad\partial_{\beta_k}\{A\}^{\frac{1}{2}}\;
=\;\frac{1}{2}A^{-\frac{1}{2}}\cdot\frac{\partial A}{\partial_{\beta_k}}$$

<br>

$$\frac{1}{2}A^{-\frac{1}{2}}\;=\;\frac{1}{2\sqrt{A}}\;=\;\frac{1}{2||y-X\beta||_2}$$

$$\frac{\partial A}{\partial_{\beta_k}}\;
=\;\frac{\partial\bigg[\frac{1}{n}\Big\{(y_1-X_{11}\beta_1)^2+(y_1-X_{12}\beta_2)^2+\;\cdots\;+(y_n-X_{n(d-1)}\beta_{d-1})^2+(y_n-X_{nd}\beta_d)^2\Big\}\bigg]}
{\partial_{\beta_k}}$$

$$=\;\frac{2}{n}\bigg\{(y_1-X_{1k}\beta_k)(-X_{1k})+\;\cdots\;+(y_n-X_{nk}\beta_k)(-X_{nk})\bigg\}$$

<br>

$$\rightarrow\quad\quad\partial_{\beta_k}||y-X\beta||_2\;=\;-\frac{X^T_{\cdot k}(y-X\beta)}{n||y-X\beta||_2}$$

$$\therefore\quad\nabla_\beta||y-X\beta||_2\;=\;-\frac{X^T(y-X\beta)}{n||y-X\beta||_2}$$

&nbsp; 주어진 레이블과, 우리가 예상한 함수 혹은 분포에 데이터를 입력한 결과( $\hat{y}$ )과의 
<span style="color: #2454ff;">**차이( Loss )를 최소화**</span>하는 방향으로 진행하면, 
예상했던 함수 혹은 분포가 점점 **실제 상황에 가까워진다**고 볼 수 있는 것이다.

$$\beta^{(t+1)}\quad\quad\leftarrow\quad\quad\beta^{(t)}-\lambda\nabla_\beta||y-X\beta^{(t)}||\;=\;\beta^{(t)}+\frac{2\lambda}{n}X^T(y-X\beta^{(t)})$$

&nbsp; 다만 여기에도 문제가 있다. 전체 데이터에 대해 계산을 진행하니, Loss function은 변하지 않는다. 그래서 학습 속도를 잘못 정하게 되면, 
**local minimam**에 빠지게 될 수도 있다. 이 뿐만 아니라, 학습 데이터가 많다면 **메모리 부족**으로 학습 자체를 진행하지 못할 수도 있다. 
그래서 나온 것이 <span style="color: #2454ff;">**확률적 경사하강법**</span>이다.

#### 확률적 경사하강법

&nbsp; 확률적 경사하강법(Stochastic Gradient Descent; SGD)은 모든 데이터에 대해 parameter를 업데이트하는 것이 아니라, 
랜덤으로 데이터의 일부만 parameter 업데이트하는데 사용하는 것이다. 그래서 기존 경사하강법에서 Loss function의 고정으로 인해 local 언덕을 넘지 못할 상황에서도, 
랜덤한 데이터 선택에 의해 Loss function이 변화하여 넘어갈 수도 있다.

&nbsp; 또한, 계산량 자체가 줄어 학습 속도적인 측면에서도 이점이다. 
물론 SGD가 만능은 아니지만 실증적으로 더 낫다고 검증된 바가 있으니, 사용하지 않을 이유가 없다.


#### Deep Learning

&nbsp; 딥러닝은 신경망을 깊게 쌓은 것이고, <span style="color: #2454ff;">**신경망은 선형모델과 활성함수를 합성한 함수**</span>이다.<br>
딥러닝으로서 의미있게 동작하기 위해서는 비선형 형태를 띄어야 하고, 따라서 **활성함수는 비선형 함수**를 사용해야 한다. Input 데이터로부터 $\hat{y}$ 을 
계산하는 과정을 <span style="color: #2454ff;">**순전파(forward-propagation)**</span>, **Loss** 를 최소화하는 방향으로 parameter 를 
업데이트 하는 과정을 <span style="color: #2454ff;">**역전파(backpropagation)**</span>라고 한다.

&nbsp; 이론적으로는 2층의 신경망으로도 임의의 연속함수를 근사할 수 있지만, 층이 깊을수록 필요한 뉴런의 개수가 줄어든다. 하지만 무작정으로 깊게하면, 
**gradient vanishing** 문제가 발생한다.

<br>

<hr>

### 피어 세션

아무래도 수식에 대한 완벽한 이해가 이루어지지 않아서 인지, 관련한 이야기가 많았다.

현재 해결을 하진 못했지만, 추후에 직접적으로 수식을 풀어서 그 궁금증을 해소할 예정이다.

- 경사하강법으로 선형회귀 계수를 구할 때, ${\partial}_{\beta_k}||y-X \beta||_2$ 를 푸는 과정에서 $\beta$ 에는 왜 k가 붙지 않는가?

<br>

<br>

<br>

<style scoped>
.tags { color: #2454ff; }
a { color: #2454ff; }
</style>